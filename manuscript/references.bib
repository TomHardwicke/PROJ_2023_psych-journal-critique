@techreport{2021,
  title = {Handling of Post-Publication Critiques},
  year = {2021},
  month = sep,
  institution = {Committee on Publication Ethics},
  doi = {10.24318/o1VgCAih},
  urldate = {2023-06-20}
}

@article{aksnes2003,
  title = {Characteristics of Highly Cited Papers},
  author = {Aksnes, Dag W},
  year = {2003},
  month = dec,
  journal = {Research Evaluation},
  volume = {12},
  number = {3},
  pages = {159--170},
  issn = {09582029, 14715449},
  doi = {10.3152/147154403781776645},
  urldate = {2023-04-14},
  langid = {english}
}

@article{alberts2015,
  title = {Self-Correction in Science at Work},
  author = {Alberts, Bruce and Cicerone, Ralph J. and Fienberg, Stephen E. and Kamb, Alexander and McNutt, Marcia and Nerem, Robert M. and Schekman, Randy and Shiffrin, Richard and Stodden, Victoria and Suresh, Subra and Zuber, Maria T. and Pope, Barbara Kline and Jamieson, Kathleen Hall},
  year = {2015},
  month = jun,
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1420--1422},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3847},
  urldate = {2023-06-20},
  abstract = {Improve incentives to support research integrity           ,                             Week after week, news outlets carry word of new scientific discoveries, but the media sometimes give suspect science equal play with substantive discoveries. Careful qualifications about what is known are lost in categorical headlines. Rare instances of misconduct or instances of irreproducibility are translated into concerns that science is broken. The October 2013               Economist               headline proclaimed ``Trouble at the lab: Scientists like to think of science as self-correcting. To an alarming degree, it is not'' (                                1                              ). Yet, that article is also rich with instances of science both policing itself, which is how the problems came to               The Economist's               attention in the first place, and addressing discovered lapses and irreproducibility concerns. In light of such issues and efforts, the U.S. National Academy of Sciences (NAS) and the Annenberg Retreat at Sunnylands convened our group to examine ways to remove some of the current disincentives to high standards of integrity in science.},
  langid = {english}
}

@article{allison2016,
  title = {Reproducibility: {{A}} Tragedy of Errors},
  shorttitle = {Reproducibility},
  author = {Allison, David B. and Brown, Andrew W. and George, Brandon J. and Kaiser, Kathryn A.},
  year = {2016},
  month = feb,
  journal = {Nature},
  volume = {530},
  number = {7588},
  pages = {27--29},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/530027a},
  urldate = {2024-06-19},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/7D4KRZUV/Allison et al. - 2016 - Reproducibility A tragedy of errors.pdf}
}

@article{altman2002,
  title = {Poor-{{Quality Medical Research}}: {{What Can Journals Do}}?},
  shorttitle = {Poor-{{Quality Medical Research}}},
  author = {Altman, Douglas G.},
  year = {2002},
  month = jun,
  journal = {JAMA},
  volume = {287},
  number = {21},
  pages = {2765},
  issn = {0098-7484},
  doi = {10.1001/jama.287.21.2765},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/BNIM7MLI/Altman - 2002 - Poor-Quality Medical Research What Can Journals D.pdf}
}

@article{altman2005,
  title = {Unjustified {{Restrictions}} on {{Letters}} to the {{Editor}}},
  author = {Altman, Douglas G},
  year = {2005},
  month = may,
  journal = {PLoS Medicine},
  volume = {2},
  number = {5},
  pages = {e126},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020126},
  urldate = {2024-04-12},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/BJD4J4FN/Altman - 2005 - Unjustified Restrictions on Letters to the Editor.pdf}
}

@article{amer2022,
  title = {Occupational {{Burnout}} and {{Productivity Loss}}: {{A Cross-Sectional Study Among Academic University Staff}}},
  shorttitle = {Occupational {{Burnout}} and {{Productivity Loss}}},
  author = {Amer, Shaimaa A. A. M. and Elotla, Sally Fawzy and Ameen, Abeer Elsayed and Shah, Jaffer and Fouad, Ahmed Mahmoud},
  year = {2022},
  month = apr,
  journal = {Frontiers in Public Health},
  volume = {10},
  pages = {861674},
  issn = {2296-2565},
  doi = {10.3389/fpubh.2022.861674},
  urldate = {2023-11-01},
  abstract = {Background               Burnout has been endorsed with serious negative health- and work-related outcomes. This study is aimed to assess the prevalence of burnout and its association with work productivity among academic staff.                                         Methods               This cross-sectional study involved 240 academic staff working at a public university in Egypt. Participants were invited to complete a web-based questionnaire involving basic personal, health, and work-related characteristics. Besides, Maslach Burnout Inventory-Human Services Survey (MBI-HSS) was used to assess occupational burnout dimensions (i.e., emotional exhaustion ``EE,'' depersonalization ``DP,'' and personal accomplishment ``PA''), while work productivity was assessed with the Health and Work Performance Questionnaire (HPQ).                                         Results               In total, 28\% of respondents scored high in EE [95\% confidence interval (CI): 22.5--33.8\%], 18.3\% high in DP (95\% CI: 13.8--3.6\%), and 88.3\% scored low in PA (95\% CI: 83.8--91.9\%). Seventy percent of respondents scored high in only one burnout dimension, 21.7\% scored high in two dimensions, while 7.1\% scored high in all three dimensions. Multivariable analysis showed that EE was the only burnout dimension that showed a statistically significant association between absenteeism and presenteeism rates. The absenteeism rates among respondents with moderate and high EE were 2.1 and 3.3 times the rates among those with low EE, respectively. Likewise, the presenteeism rates among respondents with moderate and high EE were 2.4 and 4.7 times the rates among those with low EE, respectively.                                         Conclusions               Academic staff showed a high prevalence of at least one burnout dimension. Moderate and high EE scores were significantly associated with increased productivity loss when compared to low EE.},
  file = {/Users/anniewhamond/Zotero/storage/4Q2Y6TTY/Amer et al. - 2022 - Occupational Burnout and Productivity Loss A Cros.pdf}
}

@article{anderson2007,
  title = {Normative {{Dissonance}} in {{Science}}: {{Results}} from a {{National Survey}} of {{U}}.{{S}}. {{Scientists}}},
  shorttitle = {Normative {{Dissonance}} in {{Science}}},
  author = {Anderson, Melissa S. and Martinson, Brian C. and De Vries, Raymond},
  year = {2007},
  month = dec,
  journal = {Journal of Empirical Research on Human Research Ethics},
  volume = {2},
  number = {4},
  pages = {3--14},
  issn = {1556-2646, 1556-2654},
  doi = {10.1525/jer.2007.2.4.3},
  urldate = {2023-06-20},
  abstract = {Norms of behavior in scientific research represent ideals to which most scientists subscribe. Our analysis of the extent of dissonance between these widely espoused ideals and scientists' perceptions of their own and others' behavior is based on survey responses from 3,247 mid- and early-career scientists who had research funding from the U.S. National Institutes of Health. We found substantial normative dissonance, particularly between espoused ideals and respondents' perceptions of other scientists' typical behavior. Also, respondents on average saw other scientists' behavior as more counternormative than normative. Scientists' views of their fields as cooperative or competitive were associated with their normative perspectives, with competitive fields showing more counternormative behavior. The high levels of normative dissonance documented here represent a persistent source of stress in science.},
  langid = {english}
}

@article{anderson2010,
  title = {Extending the {{Mertonian Norms}}: {{Scientists}}' {{Subscription}} to {{Norms}} of {{Research}}},
  shorttitle = {Extending the {{Mertonian Norms}}},
  author = {Anderson, Melissa S. and Ronning, Emily A. and Vries, Raymond De and Martinson, Brian C.},
  year = {2010},
  month = may,
  journal = {The Journal of Higher Education},
  volume = {81},
  number = {3},
  pages = {366--393},
  issn = {0022-1546, 1538-4640},
  doi = {10.1080/00221546.2010.11779057},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/6RE9V29S/Anderson et al. - 2010 - Extending the Mertonian Norms Scientists' Subscri.pdf}
}

@article{asendorpf2013,
  title = {Recommendations for {{Increasing Replicability}} in {{Psychology}}},
  author = {Asendorpf, Jens B. and Conner, Mark and De Fruyt, Filip and De Houwer, Jan and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and Van Aken, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
  year = {2013},
  month = mar,
  journal = {European Journal of Personality},
  volume = {27},
  number = {2},
  pages = {108--119},
  issn = {0890-2070, 1099-0984},
  doi = {10.1002/per.1919},
  urldate = {2023-06-21},
  abstract = {Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward. Copyright {\copyright} 2013 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/EJJFGX52/Asendorpf et al. - 2013 - Recommendations for Increasing Replicability in Ps.pdf}
}

@article{banks2016,
  title = {Questions {{About Questionable Research Practices}} in the {{Field}} of {{Management}}: {{A Guest Commentary}}},
  shorttitle = {Questions {{About Questionable Research Practices}} in the {{Field}} of {{Management}}},
  author = {Banks, George C. and O'Boyle, Ernest H. and Pollack, Jeffrey M. and White, Charles D. and Batchelor, John H. and Whelpley, Christopher E. and Abston, Kristie A. and Bennett, Andrew A. and Adkins, Cheryl L.},
  year = {2016},
  month = jan,
  journal = {Journal of Management},
  volume = {42},
  number = {1},
  pages = {5--20},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206315619011},
  urldate = {2023-06-21},
  abstract = {The discussion regarding questionable research practices (QRPs) in management as well as the broader natural and social sciences has increased substantially in recent years. Despite the attention, questions remain regarding research norms and the implications for both theoretical and practical advancements. The aim of the current article is to address these issues in a question-and-answer format while drawing upon both past research and the results of a series of new studies conducted using a mixed-methods design. Our goal is to encourage a systematic, collegial, and constructive dialogue regarding QRPs in management research.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/JIUSHEQ2/Banks et al. - 2016 - Questions About Questionable Research Practices in.pdf}
}

@article{banks2016a,
  title = {Editorial: {{Evidence}} on {{Questionable Research Practices}}: {{The Good}}, the {{Bad}}, and the {{Ugly}}},
  shorttitle = {Editorial},
  author = {Banks, George C. and Rogelberg, Steven G. and Woznyj, Haley M. and Landis, Ronald S. and Rupp, Deborah E.},
  year = {2016},
  month = sep,
  journal = {Journal of Business and Psychology},
  volume = {31},
  number = {3},
  pages = {323--338},
  issn = {0889-3268, 1573-353X},
  doi = {10.1007/s10869-016-9456-7},
  urldate = {2023-06-19},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/78CCMAHQ/Banks et al. - 2016 - Editorial Evidence on Questionable Research Pract.pdf}
}

@article{bastian2014,
  title = {A {{Stronger Post-Publication Culture Is Needed}} for {{Better Science}}},
  author = {Bastian, Hilda},
  year = {2014},
  month = dec,
  journal = {PLoS Medicine},
  volume = {11},
  number = {12},
  pages = {e1001772},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1001772},
  urldate = {2024-11-06},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/WRTKHPVQ/Bastian - 2014 - A Stronger Post-Publication Culture Is Needed for .pdf}
}

@article{bedeian2004,
  title = {Peer Review and the Social Construction of Knowledge in the Management Discipline},
  author = {Bedeian, Arthur G.},
  year = {2004},
  month = jun,
  journal = {Academy of Management Learning \& Education},
  volume = {3},
  number = {2},
  pages = {198--216},
  issn = {1537-260X, 1944-9585},
  doi = {10.5465/amle.2004.13500489},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/B4KZSSLY/Bedeian - 2004 - Peer Review and the Social Construction of Knowled.pdf}
}

@article{bem2011,
  title = {Feeling the Future: {{Experimental}} Evidence for Anomalous Retroactive Influences on Cognition and Affect.},
  shorttitle = {Feeling the Future},
  author = {Bem, Daryl J.},
  year = {2011},
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {407--425},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/a0021524},
  urldate = {2023-09-10},
  abstract = {The term psi denotes anomalous processes of information or energy transfer that are currently unexplained in terms of known physical or biological mechanisms. Two variants of psi are precognition (conscious cognitive awareness) and premonition (affective apprehension) of a future event that could not otherwise be anticipated through any known inferential process. Precognition and premonition are themselves special cases of a more general phenomenon: the anomalous retroactive influence of some future event on an individual's current responses, whether those responses are conscious or nonconscious, cognitive or affective. This article reports 9 experiments, involving more than 1,000 participants, that test for retroactive influence by ``time-reversing'' well-established psychological effects so that the individual's responses are obtained before the putatively causal stimulus events occur. Data are presented for 4 time-reversed effects: precognitive approach to erotic stimuli and precognitive avoidance of negative stimuli; retroactive priming; retroactive habituation; and retroactive facilitation of recall. The mean effect size (d) in psi performance across all 9 experiments was 0.22, and all but one of the experiments yielded statistically significant results. The individual-difference variable of stimulus seeking, a component of extraversion, was significantly correlated with psi performance in 5 of the experiments, with participants who scored above the midpoint on a scale of stimulus seeking achieving a mean effect size of 0.43. Skepticism about psi, issues of replication, and theories of psi are also discussed.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/LVJX6QBT/Bem - 2011 - Feeling the future Experimental evidence for anom.pdf}
}

@article{berto2020,
  title = {It Is Undeniable That {{Nature}} Has to Be Restorative to Restore Attention, Otherwise the Effect Is Unreliable},
  author = {Berto, Rita},
  year = {2020},
  month = oct,
  journal = {Journal of Environmental Psychology},
  volume = {71},
  pages = {101495},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2020.101495},
  urldate = {2023-09-10},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/III3P8ZA/Berto - 2020 - It is undeniable that Nature has to be restorative.pdf}
}

@article{besancon2021,
  title = {Open Science Saves Lives: Lessons from the {{COVID-19}} Pandemic},
  shorttitle = {Open Science Saves Lives},
  author = {Besan{\c c}on, Lonni and {Peiffer-Smadja}, Nathan and Segalas, Corentin and Jiang, Haiting and Masuzzo, Paola and Smout, Cooper and Billy, Eric and Deforet, Maxime and Leyrat, Cl{\'e}mence},
  year = {2021},
  month = jun,
  journal = {BMC medical research methodology},
  volume = {21},
  number = {1},
  pages = {117},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01304-y},
  abstract = {In the last decade Open Science principles have been successfully advocated for and are being slowly adopted in different research communities. In response to the COVID-19 pandemic many publishers and researchers have sped up their adoption of Open Science practices, sometimes embracing them fully and sometimes partially or in a sub-optimal manner. In this article, we express concerns about the violation of some of the Open Science principles and its potential impact on the quality of research output. We provide evidence of the misuses of these principles at different stages of the scientific process. We call for a wider adoption of Open Science practices in the hope that this work will encourage a broader endorsement of Open Science principles and serve as a reminder that science should always be a rigorous process, reliable and transparent, especially in the context of a pandemic where research findings are being translated into practice even more rapidly. We provide all data and scripts at https://osf.io/renxy/ .},
  langid = {english},
  pmcid = {PMC8179078},
  pmid = {34090351},
  keywords = {COVID-19,Humans,Methodology,Open science,Pandemics,Peer review,Publications,Research Personnel,SARS-CoV-2},
  file = {/Users/anniewhamond/Zotero/storage/E87BEWAC/Besançon et al. - 2021 - Open science saves lives lessons from the COVID-1.pdf}
}

@misc{besancon2021a,
  title = {Open {{Letter}}: {{Scientists}} Stand up to Protect Academic Whistleblowers and Post-Publication Peer Review.},
  shorttitle = {Open {{Letter}}},
  author = {Besan{\c c}on, Lonni and Samuel, Alexander and Sana, Thibault and Rebeaud, Mathieu Edouard and Guihur, Anthony and {Robinson-Rechavi}, Marc and Le Berre, Nicolas and Mulot, Matthieu and {Meyerowitz-Katz}, Gideon and Maisonneuve, Herve and Nosek, Brian A.},
  year = {2021},
  month = may,
  doi = {10.31219/osf.io/2awsv},
  urldate = {2024-06-11},
  abstract = {The authors and the co-signatories of this open letter are scientists dedicated to transparency and integrity of research. We support the work needed to investigate potential errors and possible misconduct and believe the scientific community can do more to protect whistleblowers against harassment and threats. Individual researchers can provide vocal support for whistleblowers and against harassment to shift norms. Journals, funders, policymakers, and institutions can make explicit policies that protect whistleblowers and establish fair, judicious, and transparent processes for addressing potential misconduct that protect all participants. With this letter, we show our support for post-publication peer-review, to Elisabeth Bik and her work, and to all the whistleblowers that help maintain quality, honesty, integrity and trustworthiness of scientific advances.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  file = {/Users/anniewhamond/Zotero/storage/74QSVF92/Besançon et al. - 2021 - Open Letter Scientists stand up to protect academ.pdf}
}

@article{besancon2022,
  title = {Correction of Scientific Literature: {{Too}} Little, Too Late!},
  shorttitle = {Correction of Scientific Literature},
  author = {Besan{\c c}on, Lonni and Bik, Elisabeth and Heathers, James and {Meyerowitz-Katz}, Gideon},
  year = {2022},
  month = mar,
  journal = {PLOS Biology},
  volume = {20},
  number = {3},
  pages = {e3001572},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3001572},
  urldate = {2023-08-27},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/CVTI9D32/Besançon et al. - 2022 - Correction of scientific literature Too little, t.pdf}
}

@article{bik2016,
  title = {The {{Prevalence}} of {{Inappropriate Image Duplication}} in {{Biomedical Research Publications}}},
  author = {Bik, Elisabeth M. and Casadevall, Arturo and Fang, Ferric C.},
  editor = {Sibley, L. David},
  year = {2016},
  month = jul,
  journal = {mBio},
  volume = {7},
  number = {3},
  pages = {e00809-16},
  issn = {2161-2129, 2150-7511},
  doi = {10.1128/mBio.00809-16},
  urldate = {2023-06-20},
  abstract = {ABSTRACT             Inaccurate data in scientific papers can result from honest error or intentional falsification. This study attempted to determine the percentage of published papers that contain inappropriate image duplication, a specific type of inaccurate data. The images from a total of 20,621 papers published in 40 scientific journals from 1995 to 2014 were visually screened. Overall, 3.8\% of published papers contained problematic figures, with at least half exhibiting features suggestive of deliberate manipulation. The prevalence of papers with problematic images has risen markedly during the past decade. Additional papers written by authors of papers with problematic images had an increased likelihood of containing problematic images as well. As this analysis focused only on one type of data, it is likely that the actual prevalence of inaccurate data in the published literature is higher. The marked variation in the frequency of problematic images among journals suggests that journal practices, such as prepublication image screening, influence the quality of the scientific literature.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/DTE8CY47/Bik et al. - 2016 - The Prevalence of Inappropriate Image Duplication .pdf}
}

@article{bik2018,
  title = {Analysis and {{Correction}} of {{Inappropriate Image Duplication}}: The {{{\emph{Molecular}}}}{\emph{ and }}{{{\emph{Cellular Biology}}}} {{Experience}}},
  shorttitle = {Analysis and {{Correction}} of {{Inappropriate Image Duplication}}},
  author = {Bik, Elisabeth M. and Fang, Ferric C. and Kullas, Amy L. and Davis, Roger J. and Casadevall, Arturo},
  year = {2018},
  month = oct,
  journal = {Molecular and Cellular Biology},
  volume = {38},
  number = {20},
  pages = {e00309-18},
  issn = {1098-5549},
  doi = {10.1128/MCB.00309-18},
  urldate = {2024-06-06},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/C94DV5YB/Bik et al. - 2018 - Analysis and Correction of Inappropriate Image Dup.pdf}
}

@article{brembs2013,
  title = {Deep Impact: Unintended Consequences of Journal Rank},
  shorttitle = {Deep Impact},
  author = {Brembs, Bj{\"o}rn and Button, Katherine and Munaf{\`o}, Marcus},
  year = {2013},
  journal = {Frontiers in Human Neuroscience},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00291},
  urldate = {2023-06-20},
  file = {/Users/anniewhamond/Zotero/storage/MBVRBIEV/Brembs et al. - 2013 - Deep impact unintended consequences of journal ra.pdf}
}

@article{brewin2023,
  title = {Inaccuracy in the {{Scientific Record}} and {{Open Postpublication Critique}}},
  author = {Brewin, Chris R.},
  year = {2023},
  month = feb,
  journal = {Perspectives on Psychological Science},
  pages = {174569162211413},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/17456916221141357},
  urldate = {2023-06-09},
  abstract = {There is growing evidence that the published psychological literature is marred by multiple errors and inaccuracies and often fails to reflect the changing nature of the knowledge base. At least four types of error are common---citation error, methodological error, statistical error, and interpretation error. In the face of the apparent inevitability of these inaccuracies, core scientific values such as openness and transparency require that correction mechanisms are readily available. In this article, I reviewed standard mechanisms in psychology journals and found them to have limitations. The effects of more widely enabling open postpublication critique in the same journal in addition to conventional peer review are considered. This mechanism is well established in medicine and the life sciences but rare in psychology and may assist psychological science to correct itself.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/EN5Q5XJN/Brewin - 2023 - Inaccuracy in the Scientific Record and Open Postp.pdf}
}

@article{button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/BNX7QM5Z/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{cafri2010,
  title = {A {{Meta-Meta-Analysis}}: {{Empirical Review}} of {{Statistical Power}}, {{Type I Error Rates}}, {{Effect Sizes}}, and {{Model Selection}} of {{Meta-Analyses Published}} in {{Psychology}}},
  shorttitle = {A {{Meta-Meta-Analysis}}},
  author = {Cafri, Guy and Kromrey, Jeffrey D. and Brannick, Michael T.},
  year = {2010},
  month = mar,
  journal = {Multivariate Behavioral Research},
  volume = {45},
  number = {2},
  pages = {239--270},
  issn = {0027-3171, 1532-7906},
  doi = {10.1080/00273171003680187},
  urldate = {2023-06-20},
  langid = {english}
}

@article{chambers2020,
  title = {Verification {{Reports}}: {{A}} New Article Type at {{Cortex}}},
  shorttitle = {Verification {{Reports}}},
  author = {Chambers, Christopher D.},
  year = {2020},
  month = aug,
  journal = {Cortex},
  volume = {129},
  pages = {A1-A3},
  issn = {00109452},
  doi = {10.1016/j.cortex.2020.04.020},
  urldate = {2023-09-10},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/IIIY4I8P/Chambers - 2020 - Verification Reports A new article type at Cortex.pdf}
}

@article{comitatoorganizzatore2022,
  title = {{Libro degli abstract}},
  author = {{Comitato Organizzatore}},
  year = {2022},
  month = dec,
  publisher = {Zenodo},
  doi = {10.5281/ZENODO.7423040},
  urldate = {2023-11-01},
  abstract = {Questo volume raccoglie 44 abstract relativi agli altrettanti lavori presentati a POLI-COVID-22, primo congresso scientifico in ambito accademico concepito per tracciare un bilancio multisettoriale della crisi COVID-19 e della sua gestione. Il programma si {\`e} articolato su cinque aree tematiche: Biologia; Medicina; Diritto; Bioetica; Sociologia e Comunicazione. Studiosi delle Istituzioni e studiosi indipendenti, sia italiani sia stranieri, sono intervenuti in presenza e in collegamento telematico per presentare lo stato dell'arte della conoscenza scientifica nei vari ambiti. Le registrazioni audiovisive sono disponibili sui canali telematici accessibili tramite i link pubblicati sul sito web del Congresso https://www.libera-scelta.it/policovid22/},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {italian},
  keywords = {COVID-19; COVID; SARS-CoV-2; pandemic; biology; medicine; bioethics; constitutional law; sociology; communication},
  file = {/Users/anniewhamond/Zotero/storage/BE65DX68/Comitato Organizzatore - 2022 - Libro degli abstract.pdf}
}

@techreport{committeeonpublicationethics2021,
  title = {Handling of Post-Publication Critiques},
  author = {{COPE}},
  year = {2021},
  month = sep,
  institution = {Committee on Publication Ethics},
  doi = {10.24318/o1VgCAih},
  urldate = {2024-11-06},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/HBPCN43B/2021 - Handling of post-publication critiques.pdf}
}

@article{delamothe2002,
  title = {Twenty Thousand Conversations},
  author = {Delamothe, T.},
  year = {2002},
  month = may,
  journal = {BMJ},
  volume = {324},
  number = {7347},
  pages = {1171--1172},
  issn = {09598138, 14685833},
  doi = {10.1136/bmj.324.7347.1171},
  urldate = {2023-06-22},
  file = {/Users/anniewhamond/Zotero/storage/7ATBULQ6/Delamothe - 2002 - Twenty thousand conversations.pdf}
}

@article{devries2018,
  title = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression},
  shorttitle = {The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments},
  author = {De Vries, Y. A. and Roest, A. M. and De Jonge, P. and Cuijpers, P. and Munaf{\`o}, M. R. and Bastiaansen, J. A.},
  year = {2018},
  month = nov,
  journal = {Psychological Medicine},
  volume = {48},
  number = {15},
  pages = {2453--2455},
  issn = {0033-2917, 1469-8978},
  doi = {10.1017/S0033291718001873},
  urldate = {2023-08-22},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/URDHWVSW/De Vries et al. - 2018 - The cumulative effect of reporting and citation bi.pdf}
}

@article{diamandis2023,
  title = {Scientific {{Journals Should Encourage}}, {{Not Hinder}}, {{Debates About Their Published Papers}}},
  author = {Diamandis, Eleftherios P.},
  year = {2023},
  month = apr,
  journal = {EJIFCC},
  volume = {34},
  number = {1},
  pages = {81--84},
  issn = {1650-3414},
  abstract = {The revolution in electronic publishing now allows for papers to be continuously critiqued through letters to the editor, online comments, tweets and other means. However, established top-ranked journals still pose serious barriers regarding cultivation, documentation and dissemination of post publication critiques (1). To improve on this situation, Hardwicke et al. published a set of rules, one being for journals to actively encourage and highlight post publication critique to their readership. In this commentary, I present a case whereby the editors of a top ranked journal hindered the discussion/debate between authors and expert readers. Highlighting and publishing such cases will likely put pressure on journals to modify their current policies and actively encourage post publication review. Like Hardwicke et al., we believe that post publication review is a major vehicle for advancing and accelerating science, by encouraging debates, resolving disagreements and revealing flaws in already published (and in many cases seemingly high-impact) papers.},
  langid = {english},
  pmcid = {PMC10131242},
  pmid = {37124652}
}

@article{dwan2008,
  title = {Systematic {{Review}} of the {{Empirical Evidence}} of {{Study Publication Bias}} and {{Outcome Reporting Bias}}},
  author = {Dwan, Kerry and Altman, Douglas G. and Arnaiz, Juan A. and Bloom, Jill and Chan, An-Wen and Cronin, Eugenia and Decullier, Evelyne and Easterbrook, Philippa J. and Von Elm, Erik and Gamble, Carrol and Ghersi, Davina and Ioannidis, John P. A. and Simes, John and Williamson, Paula R.},
  editor = {Siegfried, Nandi},
  year = {2008},
  month = aug,
  journal = {PLoS ONE},
  volume = {3},
  number = {8},
  pages = {e3081},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003081},
  urldate = {2023-11-01},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/39QKT97C/Dwan et al. - 2008 - Systematic Review of the Empirical Evidence of Stu.pdf}
}

@article{earp2015,
  title = {Replication, Falsification, and the Crisis of Confidence in Social Psychology},
  author = {Earp, Brian D. and Trafimow, David},
  year = {2015},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00621},
  urldate = {2023-06-21},
  file = {/Users/anniewhamond/Zotero/storage/ELRGB3RB/Earp and Trafimow - 2015 - Replication, falsification, and the crisis of conf.pdf}
}

@misc{epskamps.2015,
  title = {Statcheck: {{Extract}} Statistics from Articles and Recompute p Values. {{R}} Package Version 1.0.1.},
  author = {{Epskamp, S.} and {Nuijten, M. B.}},
  year = {2015}
}

@article{etz2016,
  title = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}: {{Psychology}}},
  shorttitle = {A {{Bayesian Perspective}} on the {{Reproducibility Project}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  editor = {Marinazzo, Daniele},
  year = {2016},
  month = feb,
  journal = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {e0149794},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149794},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/D7VKQSHT/Etz and Vandekerckhove - 2016 - A Bayesian Perspective on the Reproducibility Proj.pdf}
}

@article{fanelli2009,
  title = {How {{Many Scientists Fabricate}} and {{Falsify Research}}? {{A Systematic Review}} and {{Meta-Analysis}} of {{Survey Data}}},
  shorttitle = {How {{Many Scientists Fabricate}} and {{Falsify Research}}?},
  author = {Fanelli, Daniele},
  editor = {Tregenza, Tom},
  year = {2009},
  month = may,
  journal = {PLoS ONE},
  volume = {4},
  number = {5},
  pages = {e5738},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0005738},
  urldate = {2023-06-19},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/M8YV8CX2/Fanelli - 2009 - How Many Scientists Fabricate and Falsify Research.pdf}
}

@article{feest2019,
  title = {Why {{Replication Is Overrated}}},
  author = {Feest, Uljana},
  year = {2019},
  month = dec,
  journal = {Philosophy of Science},
  volume = {86},
  number = {5},
  pages = {895--905},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/705451},
  urldate = {2023-06-20},
  abstract = {Current debates about the replication crisis in psychology take it for granted that direct replication is valuable, largely focusing on its role in uncovering questionable statistical practices. This article takes a broader look at the notion of replication in psychological experiments. It is argued that all experimentation/replication involves individuation judgments and that research in experimental psychology frequently turns on probing the adequacy of such judgments. In this vein, I highlight the ubiquity of conceptual and material questions in research, arguing that replication has its place but is not as central to psychological research as it is sometimes taken to be.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/NWP4SJP9/Feest - 2019 - Why Replication Is Overrated.pdf}
}

@article{fiske2016,
  title = {A {{Call}} to {{Change Science}}'s {{Culture}} of {{Shaming}}},
  author = {Fiske, APS Past President Susan T.},
  year = {2016},
  month = oct,
  journal = {APS Observer},
  volume = {29},
  urldate = {2024-09-17},
  abstract = {In a guest column, APS Past President Susan T. Fiske calls on psychological scientists to tone down the ad hominem research critiques that are spreading across social media.},
  langid = {american},
  file = {/Users/anniewhamond/Zotero/storage/3VE3I76T/a-call-to-change-sciences-culture-of-shaming.html}
}

@article{fraley2014,
  title = {The {{N-Pact Factor}}: {{Evaluating}} the {{Quality}} of {{Empirical Journals}} with {{Respect}} to {{Sample Size}} and {{Statistical Power}}},
  shorttitle = {The {{N-Pact Factor}}},
  author = {Fraley, R. Chris and Vazire, Simine},
  editor = {Ouzounis, Christos A.},
  year = {2014},
  month = oct,
  journal = {PLoS ONE},
  volume = {9},
  number = {10},
  pages = {e109019},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0109019},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/4GNYWJPY/Fraley and Vazire - 2014 - The N-Pact Factor Evaluating the Quality of Empir.pdf}
}

@article{garfield2006,
  title = {The {{History}} and {{Meaning}} of the {{Journal Impact Factor}}},
  author = {Garfield, Eugene},
  year = {2006},
  month = jan,
  journal = {JAMA},
  volume = {295},
  number = {1},
  pages = {90},
  issn = {0098-7484},
  doi = {10.1001/jama.295.1.90},
  urldate = {2023-06-20},
  langid = {english}
}

@article{gelman2022,
  title = {Criticism as Asynchronous Collaboration: {{An}} Example from Social Science Research},
  shorttitle = {Criticism as Asynchronous Collaboration},
  author = {Gelman, Andrew},
  year = {2022},
  month = dec,
  journal = {Stat},
  volume = {11},
  number = {1},
  pages = {e464},
  issn = {2049-1573, 2049-1573},
  doi = {10.1002/sta4.464},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/838AD9IA/Gelman_2022 - Criticism as asynchronous collaboration  An example from social science research.pdf;/Users/anniewhamond/Zotero/storage/ZN7R3266/Gelman - 2022 - Criticism as asynchronous collaboration An exampl.pdf}
}

@article{gopalakrishna2022,
  title = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors: {{A}} Survey among Academic Researchers in {{The Netherlands}}},
  shorttitle = {Prevalence of Questionable Research Practices, Research Misconduct and Their Potential Explanatory Factors},
  author = {Gopalakrishna, Gowri and Ter Riet, Gerben and Vink, Gerko and Stoop, Ineke and Wicherts, Jelte M. and Bouter, Lex M.},
  editor = {F{\`a}bregues, Sergi},
  year = {2022},
  month = feb,
  journal = {PLOS ONE},
  volume = {17},
  number = {2},
  pages = {e0263023},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0263023},
  urldate = {2023-06-19},
  abstract = {Prevalence of research misconduct, questionable research practices (QRPs) and their associations with a range of explanatory factors has not been studied sufficiently among academic researchers. The National Survey on Research Integrity targeted all disciplinary fields and academic ranks in the Netherlands. It included questions about engagement in fabrication, falsification and 11 QRPs over the previous three years, and 12 explanatory factor scales. We ensured strict identity protection and used the randomized response method for questions on research misconduct. 6,813 respondents completed the survey. Prevalence of fabrication was 4.3\% (95\% CI: 2.9, 5.7) and of falsification 4.2\% (95\% CI: 2.8, 5.6). Prevalence of QRPs ranged from 0.6\% (95\% CI: 0.5, 0.9) to 17.5\% (95\% CI: 16.4, 18.7) with 51.3\% (95\% CI: 50.1, 52.5) of respondents engaging frequently in at least one QRP. Being a PhD candidate or junior researcher increased the odds of frequently engaging in at least one QRP, as did being male. Scientific norm subscription (odds ratio (OR) 0.79; 95\% CI: 0.63, 1.00) and perceived likelihood of detection by reviewers (OR 0.62, 95\% CI: 0.44, 0.88) were associated with engaging in less research misconduct. Publication pressure was associated with more often engaging in one or more QRPs frequently (OR 1.22, 95\% CI: 1.14, 1.30). We found higher prevalence of misconduct than earlier surveys. Our results suggest that greater emphasis on scientific norm subscription, strengthening reviewers in their role as gatekeepers of research quality and curbing the ``publish or perish'' incentive system promotes research integrity.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/PU92R7DB/Gopalakrishna et al. - 2022 - Prevalence of questionable research practices, res.pdf}
}

@article{green2000,
  title = {Residents' Medical Information Needs in Clinic: Are They Being Met?},
  shorttitle = {Residents' Medical Information Needs in Clinic},
  author = {Green, Michael L and Ciampi, Marc A and Ellis, Peter J},
  year = {2000},
  month = aug,
  journal = {The American Journal of Medicine},
  volume = {109},
  number = {3},
  pages = {218--223},
  issn = {00029343},
  doi = {10.1016/S0002-9343(00)00458-7},
  urldate = {2023-11-01},
  langid = {english}
}

@article{grice2017,
  title = {Four {{Bad Habits}} of {{Modern Psychologists}}},
  author = {Grice, James and Barrett, Paul and Cota, Lisa and Felix, Crystal and Taylor, Zachery and Garner, Samantha and Medellin, Eliwid and Vest, Adam},
  year = {2017},
  month = aug,
  journal = {Behavioral Sciences},
  volume = {7},
  number = {4},
  pages = {53},
  issn = {2076-328X},
  doi = {10.3390/bs7030053},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/CYCXSPRL/Grice et al. - 2017 - Four Bad Habits of Modern Psychologists.pdf}
}

@article{haffar2019,
  title = {Peer {{Review Bias}}: {{A Critical Review}}},
  shorttitle = {Peer {{Review Bias}}},
  author = {Haffar, Samir and Bazerbachi, Fateh and Murad, M. Hassan},
  year = {2019},
  month = apr,
  journal = {Mayo Clinic Proceedings},
  volume = {94},
  number = {4},
  pages = {670--676},
  issn = {00256196},
  doi = {10.1016/j.mayocp.2018.09.004},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/556WWDE9/Haffar et al. - 2019 - Peer Review Bias A Critical Review.pdf}
}

@article{hardwicke2018,
  title = {Populating the {{Data Ark}}: {{An}} Attempt to Retrieve, Preserve, and Liberate Data from the Most Highly-Cited Psychology and Psychiatry Articles},
  shorttitle = {Populating the {{Data Ark}}},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  editor = {Wicherts, Jelte M.},
  year = {2018},
  month = aug,
  journal = {PLOS ONE},
  volume = {13},
  number = {8},
  pages = {e0201856},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0201856},
  urldate = {2023-04-02},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/BCT4JT4J/Hardwicke and Ioannidis - 2018 - Populating the Data Ark An attempt to retrieve, p.pdf}
}

@article{hardwicke2018a,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  year = {2018},
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {11},
  pages = {793--796},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  urldate = {2023-06-20},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/VVDBJJNN/Hardwicke and Ioannidis - 2018 - Mapping the universe of registered reports.pdf}
}

@article{hardwicke2020,
  title = {Calibrating the {{Scientific Ecosystem Through Meta-Research}}},
  author = {Hardwicke, Tom E. and Serghiou, Stylianos and Janiaud, Perrine and Danchev, Valentin and Cr{\"u}well, Sophia and Goodman, Steven N. and Ioannidis, John P.A.},
  year = {2020},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {7},
  number = {1},
  pages = {11--37},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031219-041104},
  urldate = {2024-04-12},
  abstract = {While some scientists study insects, molecules, brains, or clouds, other scientists study science itself. Meta-research, or research-on-research, is a burgeoning discipline that investigates efficiency, quality, and bias in the scientific ecosystem, topics that have become especially relevant amid widespread concerns about the credibility of the scientific literature. Meta-research may help calibrate the scientific ecosystem toward higher standards by providing empirical evidence that informs the iterative generation and refinement of reform initiatives. We introduce a translational framework that involves ( a) identifying problems, ( b) investigating problems, ( c) developing solutions, and ( d) evaluating solutions. In each of these areas, we review key meta-research endeavors and discuss several examples of prior and ongoing work. The scientific ecosystem is perpetually evolving; the discipline of meta-research presents an opportunity to use empirical evidence to guide its development and maximize its potential.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/6C3FAX32/Hardwicke et al. - 2020 - Calibrating the Scientific Ecosystem Through Meta-.pdf}
}

@article{hardwicke2021,
  title = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{{\emph{Psychological Science}}}} : An Observational Study},
  shorttitle = {Analytic Reproducibility in Articles Receiving Open Data Badges at the Journal {{{\emph{Psychological Science}}}}},
  author = {Hardwicke, Tom E. and Bohn, Manuel and MacDonald, Kyle and Hembacher, Emily and Nuijten, Mich{\`e}le B. and Peloquin, Benjamin N. and {deMayo}, Benjamin E. and Long, Bria and Yoon, Erica J. and Frank, Michael C.},
  year = {2021},
  month = jan,
  journal = {Royal Society Open Science},
  volume = {8},
  number = {1},
  pages = {201494},
  issn = {2054-5703},
  doi = {10.1098/rsos.201494},
  urldate = {2023-11-27},
  abstract = {For any scientific report, repeating the original analyses upon the original data should yield the original outcomes. We evaluated analytic reproducibility in 25               Psychological Science               articles awarded open data badges between 2014 and 2015. Initially, 16 (64\%, 95\% confidence interval [43,81]) articles contained at least one `major numerical discrepancy' ({$>$}10\% difference) prompting us to request input from original authors. Ultimately, target values were reproducible without author involvement for 9 (36\% [20,59]) articles; reproducible with author involvement for 6 (24\% [8,47]) articles; not fully reproducible with no substantive author response for 3 (12\% [0,35]) articles; and not fully reproducible despite author involvement for 7 (28\% [12,51]) articles. Overall, 37 major numerical discrepancies remained out of 789 checked values (5\% [3,6]), but original conclusions did not appear affected. Non-reproducibility was primarily caused by unclear reporting of analytic procedures. These results highlight that open data alone is not sufficient to ensure analytic reproducibility.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/WLDBGM55/Hardwicke et al. - 2021 - Analytic reproducibility in articles receiving ope.pdf}
}

@article{hardwicke2022,
  title = {Post-Publication Critique at Top-Ranked Journals across Scientific Disciplines: A Cross-Sectional Assessment of Policies and Practice},
  shorttitle = {Post-Publication Critique at Top-Ranked Journals across Scientific Disciplines},
  author = {{Hardwicke} and {Thibault, Robert T.} and Kosie, Jessica E. and Tzavella, Loukia and Bendixen, Theiss and Handcock, Sarah A. and K{\"o}neke, Vivian E. and Ioannidis, John P. A.},
  year = {2022},
  month = aug,
  journal = {Royal Society Open Science},
  volume = {9},
  number = {8},
  pages = {220139},
  issn = {2054-5703},
  doi = {10.1098/rsos.220139},
  urldate = {2023-04-02},
  abstract = {Journals exert considerable control over letters, commentaries and online comments that criticize prior research (post-publication critique). We assessed policies (Study One) and practice (Study Two) related to post-publication critique at 15 top-ranked journals in each of 22 scientific disciplines (               N               = 330 journals). Two-hundred and seven (63\%) journals accepted post-publication critique and often imposed limits on length (median 1000, interquartile range (IQR) 500--1200 words) and time-to-submit (median 12, IQR 4--26 weeks). The most restrictive limits were 175 words and two weeks; some policies imposed no limits. Of 2066 randomly sampled research articles published in 2018 by journals accepting post-publication critique, 39 (1.9\%, 95\% confidence interval [1.4, 2.6]) were linked to at least one post-publication critique (there were 58 post-publication critiques in total). Of the 58 post-publication critiques, 44 received an author reply, of which 41 asserted that original conclusions were unchanged. Clinical Medicine had the most active culture of post-publication critique: all journals accepted post-publication critique and published the most post-publication critique overall, but also imposed the strictest limits on length (median 400, IQR 400--550 words) and time-to-submit (median 4, IQR 4--6 weeks). Our findings suggest that top-ranked academic journals often pose serious barriers to the cultivation, documentation and dissemination of post-publication critique.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/MN84397Y/Hardwicke et al. - 2022 - Post-publication critique at top-ranked journals a.pdf}
}

@article{hardwicke2022a,
  title = {Estimating the {{Prevalence}} of {{Transparency}} and {{Reproducibility-Related Research Practices}} in {{Psychology}} (2014--2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {239--251},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620979806},
  urldate = {2023-06-09},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/CQZ93AXD/Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Repr.pdf}
}

@article{hicks2020,
  title = {Restoration of Sustained Attention Following Virtual Nature Exposure: {{Undeniable}} or Unreliable?},
  shorttitle = {Restoration of Sustained Attention Following Virtual Nature Exposure},
  author = {Hicks, Lydia J. and Smith, Alyssa C. and Ralph, Brandon C.W. and Smilek, Daniel},
  year = {2020},
  month = oct,
  journal = {Journal of Environmental Psychology},
  volume = {71},
  pages = {101488},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2020.101488},
  urldate = {2023-09-10},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/MXKBJVFM/Hicks et al. - 2020 - Restoration of sustained attention following virtu.pdf;/Users/anniewhamond/Zotero/storage/YC3HZ2UA/PPCTarget Article.pdf}
}

@article{ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  year = {2005},
  month = aug,
  journal = {PLoS Medicine},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/6JJ7CSQE/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{ioannidis2022,
  title = {Massive Covidization of Research Citations and the Citation Elite},
  author = {Ioannidis, John P. A. and Bendavid, Eran and {Salholz-Hillel}, Maia and Boyack, Kevin W. and Baas, Jeroen},
  year = {2022},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {28},
  pages = {e2204074119},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2204074119},
  urldate = {2023-04-02},
  abstract = {Massive scientific productivity accompanied the COVID-19 pandemic. We evaluated the citation impact of COVID-19 publications relative to all scientific work published in 2020 to 2021 and assessed the impact on scientist citation profiles. Using Scopus data until August 1, 2021, COVID-19 items accounted for 4\% of papers published, 20\% of citations received to papers published in 2020 to 2021, and {$>$}30\% of citations received in 36 of the 174 disciplines of science (up to 79.3\% in general and internal medicine). Across science, 98 of the 100 most-cited papers published in 2020 to 2021 were related to COVID-19; 110 scientists received {$\geq$}10,000 citations for COVID-19 work, but none received {$\geq$}10,000 citations for non--COVID-19 work published in 2020 to 2021. For many scientists, citations to their COVID-19 work already accounted for more than half of their total career citation count. Overall, these data show a strong covidization of research citations across science, with major impact on shaping the citation elite.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/QLD7SNWP/Ioannidis et al. - 2022 - Massive covidization of research citations and the.pdf}
}

@article{ioannidis2022a,
  title = {{{COVID-19}} Models and Expectations - {{Learning}} from the Pandemic},
  author = {Ioannidis, John P. A. and Powis, Stephen H.},
  year = {2022},
  month = dec,
  journal = {Advances in Biological Regulation},
  volume = {86},
  pages = {100922},
  issn = {2212-4934},
  doi = {10.1016/j.jbior.2022.100922},
  langid = {english},
  pmcid = {PMC9546779},
  pmid = {36241518},
  keywords = {COVID-19,Humans,Motivation,Pandemics},
  file = {/Users/anniewhamond/Zotero/storage/5HLTSJXW/Ioannidis and Powis - 2022 - COVID-19 models and expectations - Learning from t.pdf}
}

@article{ioannidis2023,
  title = {Inverse Publication Reporting Bias Favouring Null, Negative Results},
  author = {Ioannidis, John P A},
  year = {2023},
  month = jun,
  journal = {BMJ Evidence-Based Medicine},
  pages = {bmjebm-2023-112292},
  issn = {2515-446X, 2515-4478},
  doi = {10.1136/bmjebm-2023-112292},
  urldate = {2023-06-20},
  langid = {english}
}

@article{jefferson2023,
  title = {Physical Interventions to Interrupt or Reduce the Spread of Respiratory Viruses},
  author = {Jefferson, Tom and Dooley, Liz and Ferroni, Eliana and {Al-Ansary}, Lubna A and Van Driel, Mieke L and Bawazeer, Ghada A and Jones, Mark A and Hoffmann, Tammy C and Clark, Justin and Beller, Elaine M and Glasziou, Paul P and Conly, John M},
  editor = {{Cochrane Acute Respiratory Infections Group}},
  year = {2023},
  month = jan,
  journal = {Cochrane Database of Systematic Reviews},
  volume = {2023},
  number = {4},
  issn = {14651858},
  doi = {10.1002/14651858.CD006207.pub6},
  urldate = {2023-09-11},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/KSC3IIW3/Jefferson et al. - 2023 - Physical interventions to interrupt or reduce the .pdf}
}

@article{john2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611430953},
  urldate = {2023-06-19},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/CGKI8N83/John et al. - 2012 - Measuring the Prevalence of Questionable Research .pdf}
}

@article{kambhampati2020,
  title = {Unprecedented Surge in Publications Related to {{COVID-19}} in the First Three Months of Pandemic: {{A}} Bibliometric Analytic Report},
  shorttitle = {Unprecedented Surge in Publications Related to {{COVID-19}} in the First Three Months of Pandemic},
  author = {Kambhampati, Srinivas B.S. and Vaishya, Raju and Vaish, Abhishek},
  year = {2020},
  month = may,
  journal = {Journal of Clinical Orthopaedics and Trauma},
  volume = {11},
  pages = {S304-S306},
  issn = {09765662},
  doi = {10.1016/j.jcot.2020.04.030},
  urldate = {2023-11-01},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/BUIU8AD3/Kambhampati et al. - 2020 - Unprecedented surge in publications related to COV.pdf}
}

@article{khatter2021,
  title = {Is Rapid Scientific Publication Also High Quality? {{Bibliometric}} Analysis of Highly Disseminated {{{\textsc{COVID}}}} -19 Research Papers},
  shorttitle = {Is Rapid Scientific Publication Also High Quality?},
  author = {Khatter, Amandeep and Naughton, Michael and Dambha-Miller, Hajira and Redmond, Patrick},
  year = {2021},
  month = oct,
  journal = {Learned Publishing},
  volume = {34},
  number = {4},
  pages = {568--577},
  issn = {0953-1513, 1741-4857},
  doi = {10.1002/leap.1403},
  urldate = {2023-11-01},
  abstract = {Abstract                            The impact of COVID-19 has underlined the need for reliable information to guide clinical practice and policy. This urgency has to be balanced against disruption to journal handling capacity and the continued need to ensure scientific rigour. We examined the reporting quality of highly disseminated COVID-19 research papers using a bibliometric analysis examining reporting quality and risk of bias (RoB) amongst 250 top scoring Altmetric Attention Score (AAS) COVID-19 research papers between January and April 2020. Method-specific RoB tools were used to assess quality. After exclusions, 84 studies from 44 journals were included. Forty-three (51\%) were case series/studies, and only one was an randomized controlled trial. Most authors were from institutions based in China (               n               ~=\,44, 52\%). The median AAS and impact factor was 2015 (interquartile range [IQR] 1,105--4,051.5) and 12.8 (IQR 5--44.2) respectively. Nine studies (11\%) utilized a formal reporting framework, 62 (74\%) included a funding statement, and 41 (49\%) were at high RoB. This review of the most widely disseminated COVID-19 studies highlights a preponderance of low-quality case series with few research papers adhering to good standards of reporting. It emphasizes the need for cautious interpretation of research and the increasingly vital responsibility that journals have in ensuring high-quality publications.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/LI7ILNGX/Khatter et al. - 2021 - Is rapid scientific publication also high quality.pdf}
}

@article{kriegeskorte2012,
  title = {An Emerging Consensus for Open Evaluation: 18 Visions for the Future of Scientific Publishing},
  shorttitle = {An Emerging Consensus for Open Evaluation},
  author = {Kriegeskorte, Nikolaus and Walther, Alexander and Deca, Diana},
  year = {2012},
  journal = {Frontiers in Computational Neuroscience},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00094},
  urldate = {2024-04-18},
  file = {/Users/anniewhamond/Zotero/storage/YEDGTHGV/Kriegeskorte et al. - 2012 - An emerging consensus for open evaluation 18 visi.pdf}
}

@article{kvarven2019,
  title = {Comparing Meta-Analyses and Preregistered Multiple-Laboratory Replication Projects},
  author = {Kvarven, Amanda and Str{\o}mland, Eirik and Johannesson, Magnus},
  year = {2019},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {4},
  pages = {423--434},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0787-z},
  urldate = {2023-11-28},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/CTSURH5I/Kvarven et al. - 2019 - Comparing meta-analyses and preregistered multiple.pdf}
}

@article{ledford2020,
  title = {High-Profile Coronavirus Retractions Raise Concerns about Data Oversight},
  author = {Ledford, Heidi and Van Noorden, Richard},
  year = {2020},
  month = jun,
  journal = {Nature},
  volume = {582},
  number = {7811},
  pages = {160--160},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-020-01695-w},
  urldate = {2023-10-31},
  langid = {english}
}

@article{lesne2006,
  title = {A Specific Amyloid-{$\beta$} Protein Assembly in the Brain Impairs Memory},
  author = {Lesn{\'e}, Sylvain and Koh, Ming Teng and Kotilinek, Linda and Kayed, Rakez and Glabe, Charles G. and Yang, Austin and Gallagher, Michela and Ashe, Karen H.},
  year = {2006},
  month = mar,
  journal = {Nature},
  volume = {440},
  number = {7082},
  pages = {352--357},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature04533},
  urldate = {2023-06-20},
  langid = {english}
}

@article{lilienfeld2020,
  title = {Psychological Measurement and the Replication Crisis: {{Four}} Sacred Cows.},
  shorttitle = {Psychological Measurement and the Replication Crisis},
  author = {Lilienfeld, Scott O. and Strother, Adele N.},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology / Psychologie canadienne},
  volume = {61},
  number = {4},
  pages = {281--288},
  issn = {1878-7304, 0708-5591},
  doi = {10.1037/cap0000236},
  urldate = {2023-11-01},
  langid = {english}
}

@article{marcoci2022,
  title = {Reimagining Peer Review as an Expert Elicitation Process},
  author = {Marcoci, Alexandru and Vercammen, Ans and Bush, Martin and Hamilton, Daniel G. and Hanea, Anca and Hemming, Victoria and Wintle, Bonnie C. and Burgman, Mark and Fidler, Fiona},
  year = {2022},
  month = apr,
  journal = {BMC Research Notes},
  volume = {15},
  number = {1},
  pages = {127},
  issn = {1756-0500},
  doi = {10.1186/s13104-022-06016-0},
  urldate = {2023-06-20},
  abstract = {Abstract             Journal peer review regulates the flow of ideas through an academic discipline and thus has the power to shape what a research community knows, actively investigates, and recommends to policymakers and the wider public. We might assume that editors can identify the `best' experts and rely on them for peer review. But decades of research on both expert decision-making and peer review suggests they cannot. In the absence of a clear criterion for demarcating reliable, insightful, and accurate expert assessors of research quality, the best safeguard against unwanted biases and uneven power distributions is to introduce greater transparency and structure into the process. This paper argues that peer review would therefore benefit from applying a series of evidence-based recommendations from the empirical literature on structured expert elicitation. We highlight individual and group characteristics that contribute to higher quality judgements, and elements of elicitation protocols that reduce bias, promote constructive discussion, and enable opinions to be objectively and transparently aggregated.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/UIF38RNA/Marcoci et al. - 2022 - Reimagining peer review as an expert elicitation p.pdf}
}

@article{marwick2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1375986},
  urldate = {2023-11-30},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/NZPKSBJA/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf}
}

@article{mayden2012,
  title = {Peer {{Review}}: {{Publication}}'s {{Gold Standard}}},
  shorttitle = {Peer {{Review}}},
  author = {Mayden, Kelley D.},
  year = {2012},
  month = mar,
  journal = {Journal of the Advanced Practitioner in Oncology},
  volume = {3},
  number = {2},
  pages = {117--122},
  issn = {2150-0878},
  langid = {english},
  pmcid = {PMC4093306},
  pmid = {25059293}
}

@article{mckiernan2019,
  title = {Use of the {{Journal Impact Factor}} in Academic Review, Promotion, and Tenure Evaluations},
  author = {McKiernan, Erin C and Schimanski, Lesley A and Mu{\~n}oz Nieves, Carol and Matthias, Lisa and Niles, Meredith T and Alperin, Juan P},
  year = {2019},
  month = jul,
  journal = {eLife},
  volume = {8},
  pages = {e47338},
  issn = {2050-084X},
  doi = {10.7554/eLife.47338},
  urldate = {2023-11-01},
  abstract = {We analyzed how often and in what ways the Journal Impact Factor (JIF) is currently used in review, promotion, and tenure (RPT) documents of a representative sample of universities from the United States and Canada. 40\% of research-intensive institutions and 18\% of master's institutions mentioned the JIF, or closely related terms. Of the institutions that mentioned the JIF, 87\% supported its use in at least one of their RPT documents, 13\% expressed caution about its use, and none heavily criticized it or prohibited its use. Furthermore, 63\% of institutions that mentioned the JIF associated the metric with quality, 40\% with impact, importance, or significance, and 20\% with prestige, reputation, or status. We conclude that use of the JIF is encouraged in RPT evaluations, especially at research-intensive universities, and that there is work to be done to avoid the potential misuse of metrics like the JIF.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/QXHHNJUM/McKiernan et al. - 2019 - Use of the Journal Impact Factor in academic revie.pdf}
}

@article{miller2001,
  title = {Misunderstanding Analysis of Covariance.},
  author = {Miller, Gregory A. and Chapman, Jean P.},
  year = {2001},
  month = feb,
  journal = {Journal of Abnormal Psychology},
  volume = {110},
  number = {1},
  pages = {40--48},
  issn = {1939-1846, 0021-843X},
  doi = {10.1037/0021-843X.110.1.40},
  urldate = {2023-06-20},
  langid = {english}
}

@article{moncrieff2022,
  title = {The Serotonin Theory of Depression: A Systematic Umbrella Review of the Evidence},
  shorttitle = {The Serotonin Theory of Depression},
  author = {Moncrieff, Joanna and Cooper, Ruth E. and Stockmann, Tom and Amendola, Simone and Hengartner, Michael P. and Horowitz, Mark A.},
  year = {2022},
  month = jul,
  journal = {Molecular Psychiatry},
  issn = {1359-4184, 1476-5578},
  doi = {10.1038/s41380-022-01661-0},
  urldate = {2023-06-25},
  abstract = {Abstract                            The serotonin hypothesis of depression is still influential. We aimed to synthesise and evaluate evidence on whether depression is associated with lowered serotonin concentration or activity in a systematic umbrella review of the principal relevant areas of research. PubMed, EMBASE and PsycINFO were searched using terms appropriate to each area of research, from their inception until December 2020. Systematic reviews, meta-analyses and large data-set analyses in the following areas were identified: serotonin and serotonin metabolite, 5-HIAA, concentrations in body fluids; serotonin 5-HT               1A               receptor binding; serotonin transporter (SERT) levels measured by imaging or at post-mortem; tryptophan depletion studies; SERT gene associations and SERT gene-environment interactions. Studies of depression associated with physical conditions and specific subtypes of depression (e.g. bipolar depression) were excluded. Two independent reviewers extracted the data and assessed the quality of included studies using the AMSTAR-2, an adapted AMSTAR-2, or the STREGA for a large genetic study. The certainty of study results was assessed using a modified version of the GRADE. We did not synthesise results of individual meta-analyses because they included overlapping studies. The review was registered with PROSPERO (CRD42020207203). 17 studies were included: 12 systematic reviews and meta-analyses, 1 collaborative meta-analysis, 1 meta-analysis of large cohort studies, 1 systematic review and narrative synthesis, 1 genetic association study and 1 umbrella review. Quality of reviews was variable with some genetic studies of high quality. Two meta-analyses of overlapping studies examining the serotonin metabolite, 5-HIAA, showed no association with depression (largest               n               \,=\,1002). One meta-analysis of cohort studies of plasma serotonin showed no relationship with depression, and evidence that lowered serotonin concentration was associated with antidepressant use (               n               \,=\,1869). Two meta-analyses of overlapping studies examining the 5-HT               1A               receptor (largest               n               \,=\,561), and three meta-analyses of overlapping studies examining SERT binding (largest               n               \,=\,1845) showed weak and inconsistent evidence of reduced binding in some areas, which would be consistent with increased synaptic availability of serotonin in people with depression, if this was the original, causal abnormaly. However,~effects of prior antidepressant use were not reliably excluded. One meta-analysis of tryptophan depletion studies found no effect in most healthy volunteers (               n               \,=\,566), but weak evidence of an effect in those with a family history of depression (               n               \,=\,75). Another systematic review (               n               \,=\,342) and a sample of ten subsequent studies (               n               \,=\,407) found no effect in volunteers. No systematic review of tryptophan depletion studies has been performed since 2007. The two largest and highest quality studies of the SERT gene, one genetic association study (               n               \,=\,115,257) and one collaborative meta-analysis (               n               \,=\,43,165), revealed no evidence of an association with depression, or of an interaction between genotype, stress and depression. The main areas of serotonin research provide no consistent evidence of there being an association between serotonin and depression, and no support for the hypothesis that depression is caused by lowered serotonin activity or concentrations. Some evidence was consistent with the possibility that long-term antidepressant use reduces serotonin concentration.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/NJ76E2SS/Moncrieff et al. - 2022 - The serotonin theory of depression a systematic u.pdf}
}

@article{munafo2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munaf{\`o}, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and {Percie du Sert}, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  year = {2017},
  month = jan,
  journal = {Nature Human Behaviour},
  volume = {1},
  number = {1},
  pages = {0021},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  urldate = {2023-04-02},
  abstract = {Abstract             Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/V6PSVCDX/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@misc{ncbiinsights2018,
  title = {{{NCBI Insights}}: {{PubMed Commons}} to Be {{Discontinued}}},
  author = {{NCBI Insights}},
  year = {2018}
}

@article{nosek2012,
  title = {Scientific {{Utopia}}: {{I}}. {{Opening Scientific Communication}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and {Bar-Anan}, Yoav},
  year = {2012},
  month = jul,
  journal = {Psychological Inquiry},
  volume = {23},
  number = {3},
  pages = {217--243},
  issn = {1047-840X, 1532-7965},
  doi = {10.1080/1047840X.2012.692215},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/QFQZXZVZ/Nosek and Bar-Anan - 2012 - Scientific Utopia I. Opening Scientific Communica.pdf}
}

@article{nosek2012a,
  title = {Scientific {{Utopia}}: {{II}}. {{Restructuring Incentives}} and {{Practices}} to {{Promote Truth Over Publishability}}},
  shorttitle = {Scientific {{Utopia}}},
  author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {615--631},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612459058},
  urldate = {2023-06-20},
  abstract = {An academic scientist's professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive---getting it right---competitive with the more tangible and concrete incentive---getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/NPBA3R2T/Nosek et al. - 2012 - Scientific Utopia II. Restructuring Incentives an.pdf}
}

@article{nosek2015,
  title = {Promoting an Open Research Culture},
  author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and {Mayo-Wilson}, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
  year = {2015},
  month = jun,
  journal = {Science},
  volume = {348},
  number = {6242},
  pages = {1422--1425},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab2374},
  urldate = {2023-06-22},
  abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility           ,                             Transparency, openness, and reproducibility are readily recognized as vital features of science (                                1                              ,                                2                              ). When asked, most scientists embrace these features as disciplinary norms and values (                                3                              ). Therefore, one might expect that these valued features would be routine in daily practice. Yet, a growing body of evidence suggests that this is not the case (                                4                              --                                6                              ).},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/95DGKZWK/Nosek et al. - 2015 - Promoting an open research culture.pdf}
}

@article{nosek2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {719--748},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-020821-114157},
  urldate = {2023-06-20},
  abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/LWZWUXVT/Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}

@article{nuijten2016,
  title = {The Prevalence of Statistical Reporting Errors in Psychology (1985--2013)},
  author = {Nuijten, Mich{\`e}le B. and Hartgerink, Chris H. J. and Van Assen, Marcel A. L. M. and Epskamp, Sacha and Wicherts, Jelte M.},
  year = {2016},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {48},
  number = {4},
  pages = {1205--1226},
  issn = {1554-3528},
  doi = {10.3758/s13428-015-0664-2},
  urldate = {2023-06-19},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/3TAARF7L/Nuijten et al. - 2016 - The prevalence of statistical reporting errors in .pdf}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  urldate = {2023-06-20},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/WF4U7TWB/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{p.simmons2021,
  title = {Pre-registration: {{Why}} and {{How}}},
  shorttitle = {Pre-registration},
  author = {P. Simmons, Joseph and D. Nelson, Leif and Simonsohn, Uri},
  year = {2021},
  month = jan,
  journal = {Journal of Consumer Psychology},
  volume = {31},
  number = {1},
  pages = {151--162},
  issn = {1057-7408, 1532-7663},
  doi = {10.1002/jcpy.1208},
  urldate = {2023-09-27},
  abstract = {In this article, we (1) discuss the reasons why pre-registration is a good idea, both for the field and individual researchers, (2) respond to arguments against pre-registration, (3) describe how to best write and review a pre-registration, and (4) comment on pre-registration's rapidly accelerating popularity. Along the way, we describe the (big) problem that pre-registration can solve (i.e., false positives caused by p-hacking), while also offering viable solutions to the problems that pre-registration cannot solve (e.g., hidden confounds or fraud). Pre-registration does not guarantee that every published finding will be true, but without it you can safely bet that many more will be false. It is time for our field to embrace pre-registration, while taking steps to ensure that it is done right.           ,              This article is part of a Research Dialogue:                            Krishna (2021):               https://doi.org/10.1002/jcpy.1211                                         Pham \& Oh (2021):               https://doi.org/10.1002/jcpy.1209                                         Simmons et al. (2021):               https://doi.org/10.1002/jcpy.1207                                         Pham \& Oh (2021):               https://doi.org/10.1002/jcpy.1213},
  langid = {english}
}

@article{pashler2012,
  title = {Editors' {{Introduction}} to the {{Special Section}} on {{Replicability}} in {{Psychological Science}}: {{A Crisis}} of {{Confidence}}?},
  shorttitle = {Editors' {{Introduction}} to the {{Special Section}} on {{Replicability}} in {{Psychological Science}}},
  author = {Pashler, Harold and Wagenmakers, Eric--Jan},
  year = {2012},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {7},
  number = {6},
  pages = {528--530},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691612465253},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/7NMN6SZC/Pashler and Wagenmakers - 2012 - Editors’ Introduction to the Special Section on Re.pdf}
}

@article{protzko2023,
  title = {High Replicability of Newly Discovered Social-Behavioural Findings Is Achievable},
  author = {Protzko, John and Krosnick, Jon and Nelson, Leif and Nosek, Brian A. and Axt, Jordan and Berent, Matt and Buttrick, Nicholas and DeBell, Matthew and Ebersole, Charles R. and Lundmark, Sebastian and MacInnis, Bo and O'Donnell, Michael and Perfecto, Hannah and Pustejovsky, James E. and Roeder, Scott S. and Walleczek, Jan and Schooler, Jonathan W.},
  year = {2023},
  month = nov,
  journal = {Nature Human Behaviour},
  issn = {2397-3374},
  doi = {10.1038/s41562-023-01749-9},
  urldate = {2023-11-28},
  abstract = {Abstract                            Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using rigour-enhancing practices: confirmatory tests, large sample sizes, preregistration and methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50\%, replication attempts here produced the expected effects with significance testing (               P               \,{$<$}\,0.05) in 86\% of attempts, slightly exceeding the maximum expected replicability based on observed effect sizes and sample sizes. When one lab attempted to replicate an effect discovered by another lab, the effect size in the replications was 97\% that in the original study. This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/4LDV2ZTK/Protzko et al. - 2023 - High replicability of newly discovered social-beha.pdf}
}

@incollection{robertk.merton1973,
  title = {The Normative Structure of Science.},
  booktitle = {The {{Sociology}} of {{Science}}: {{Theoretical}} and {{Empirical Investigations}}.},
  author = {Merton, Robert K.},
  year = {1973},
  pages = {267--278},
  publisher = {University of Chicago Press},
  address = {Chicago, IL},
  isbn = {978-0-226-52092-6},
  file = {/Users/anniewhamond/Zotero/storage/6XSY6XWY/Robert K. Merton - 1973 - The normative structure of science..pdf;/Users/anniewhamond/Zotero/storage/XLLDZ4FV/Robert K. Merton - 1973 - The normative structure of science..pdf}
}

@article{rothman2018,
  title = {Planning {{Study Size Based}} on {{Precision Rather Than Power}}:},
  shorttitle = {Planning {{Study Size Based}} on {{Precision Rather Than Power}}},
  author = {Rothman, Kenneth J. and Greenland, Sander},
  year = {2018},
  month = sep,
  journal = {Epidemiology},
  volume = {29},
  number = {5},
  pages = {599--603},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000000876},
  urldate = {2023-10-24},
  langid = {english}
}

@techreport{sabel2023,
  type = {Preprint},
  title = {Fake {{Publications}} in {{Biomedical Science}}: {{Red-flagging Method Indicates Mass Production}}},
  shorttitle = {Fake {{Publications}} in {{Biomedical Science}}},
  author = {Sabel, Bernhard A. and Knaack, Emely and Gigerenzer, Gerd and Bilc, Mirela},
  year = {2023},
  month = may,
  institution = {Medical Ethics},
  doi = {10.1101/2023.05.06.23289563},
  urldate = {2023-06-19},
  abstract = {ABSTRACT                        Background             Integrity of academic publishing is increasingly undermined by fake science publications massively produced by commercial ``editing services'' (so-called ``paper mills''). They use AI-supported, automated production techniques at scale and sell fake publications to students, scientists, and physicians under pressure to advance their careers. Because the scale of fake publications in biomedicine is unknown, we developed a simple method to red-flag them and estimate their number.                                   Methods                            To identify indicators able to red-flagged fake publications (RFPs), we sent questionnaires to authors. Based on author responses, three               indicators               were identified: ``author's private email'', ``international co-author'' and ``hospital affiliation''. These were used to analyze 15,120 PubMed{\textregistered}-listed publications regarding date, journal, impact factor, and country of author and validated in a sample of 400 known fakes and 400 matched presumed non-fakes using classification (tallying) rules to red-flag potential fakes. For a subsample of 80 papers we used an additional indicator related to the percentage of RFP citations.                                                Results             The classification rules using two (three) indicators had sensitivities of 86\% (90\%) and false alarm rates of 44\% (37\%). From 2010 to 2020 the RFP rate increased from 16\% to 28\%. Given the 1.3 million biomedical Scimago-listed publications in 2020, we estimate the scope of {$>$}300,000 RFPs annually. Countries with the highest RFP proportion are Russia, Turkey, China, Egypt, and India (39\%-48\%), with China, in absolute terms, as the largest contributor of all RFPs (55\%).                                   Conclusions             Potential fake publications can be red-flagged using simple-to-use, validated classification rules to earmark them for subsequent scrutiny. RFP rates are increasing, suggesting higher actual fake rates than previously reported. The scale and proliferation of fake publications in biomedicine can damage trust in science, endanger public health, and impact economic spending and security. Easy-to-apply fake detection methods, as proposed here, or more complex automated methods can help prevent further damage to the permanent scientific record and enable the retraction of fake publications at scale.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/L2WC88SR/Sabel et al. - 2023 - Fake Publications in Biomedical Science Red-flagg.pdf}
}

@article{schriger2010,
  title = {Inadequate Post-Publication Review of Medical Research},
  author = {Schriger, D. L. and Altman, D. G.},
  year = {2010},
  month = aug,
  journal = {BMJ},
  volume = {341},
  number = {aug11 3},
  pages = {c3803-c3803},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.c3803},
  urldate = {2023-11-01},
  langid = {english}
}

@article{simmons2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  urldate = {2023-04-02},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/DTYY7J9S/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@article{smith2006,
  title = {Peer {{Review}}: {{A Flawed Process}} at the {{Heart}} of {{Science}} and {{Journals}}},
  shorttitle = {Peer {{Review}}},
  author = {Smith, Richard},
  year = {2006},
  month = apr,
  journal = {Journal of the Royal Society of Medicine},
  volume = {99},
  number = {4},
  pages = {178--182},
  issn = {0141-0768, 1758-1095},
  doi = {10.1177/014107680609900414},
  urldate = {2023-06-21},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/ZU4J3T5B/Smith - 2006 - Peer Review A Flawed Process at the Heart of Scie.pdf}
}

@article{soderberg2021,
  title = {Initial Evidence of Research Quality of Registered Reports Compared with the Standard Publishing Model},
  author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M. and Nosek, Brian A.},
  year = {2021},
  month = jun,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {8},
  pages = {990--997},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01142-4},
  urldate = {2023-11-30},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/S8Z89MTC/Soderberg et al. - 2021 - Initial evidence of research quality of registered.pdf}
}

@article{vandijk2010,
  title = {What {{Are}} the {{Barriers}} to {{Residents' Practicing Evidence-Based Medicine}}? {{A Systematic Review}}:},
  shorttitle = {What {{Are}} the {{Barriers}} to {{Residents' Practicing Evidence-Based Medicine}}?},
  author = {Van Dijk, Nynke and Hooft, Lotty and {Wieringa-de Waard}, Margreet},
  year = {2010},
  month = jul,
  journal = {Academic Medicine},
  volume = {85},
  number = {7},
  pages = {1163--1170},
  issn = {1040-2446},
  doi = {10.1097/ACM.0b013e3181d4152f},
  urldate = {2023-11-01},
  langid = {english}
}

@article{vazire2017,
  title = {Quality {{Uncertainty Erodes Trust}} in {{Science}}},
  author = {Vazire, Simine},
  year = {2017},
  month = jan,
  journal = {Collabra: Psychology},
  volume = {3},
  number = {1},
  pages = {1},
  issn = {2474-7394},
  doi = {10.1525/collabra.74},
  urldate = {2023-04-02},
  abstract = {When consumers of science (readers and reviewers) lack relevant details about the study design, data, and analyses, they cannot adequately evaluate the strength of a scientific study. Lack of transparency is common in science, and is encouraged by journals that place more emphasis on the aesthetic appeal of a manuscript than the robustness of its scientific claims. In doing this, journals are implicitly encouraging authors to do whatever it takes to obtain eye-catching results. To achieve this, researchers can use common research practices that beautify results at the expense of the robustness of those results (e.g., p-hacking). The problem is not engaging in these practices, but failing to disclose them. A car whose carburetor is duct-taped to the rest of the car might work perfectly fine, but the buyer has a right to know about the duct-taping. Without high levels of transparency in scientific publications, consumers of scientific manuscripts are in a similar position as buyers of used cars -- they cannot reliably tell the difference between lemons and high quality findings. This phenomenon -- quality uncertainty -- has been shown to erode trust in economic markets, such as the used car market. The same problem threatens to erode trust in science. The solution is to increase transparency and give consumers of scientific research the information they need to accurately evaluate research. Transparency would also encourage researchers to be more careful in how they conduct their studies and write up their results. To make this happen, we must tie journals' reputations to their practices regarding transparency. Reviewers hold a great deal of power to make this happen, by demanding the transparency needed to rigorously evaluate scientific manuscripts. The public expects transparency from science, and appropriately so -- we should be held to a higher standard than used car salespeople.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/XL74QS2N/Vazire - 2017 - Quality Uncertainty Erodes Trust in Science.pdf}
}

@article{vazire2018,
  title = {Implications of the {{Credibility Revolution}} for {{Productivity}}, {{Creativity}}, and {{Progress}}},
  author = {Vazire, Simine},
  year = {2018},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {13},
  number = {4},
  pages = {411--417},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691617751884},
  urldate = {2023-06-20},
  abstract = {The credibility revolution (sometimes referred to as the ``replicability crisis'') in psychology has brought about many changes in the standards by which psychological science is evaluated. These changes include (a) greater emphasis on transparency and openness, (b) a move toward preregistration of research, (c) more direct-replication studies, and (d) higher standards for the quality and quantity of evidence needed to make strong scientific claims. What are the implications of these changes for productivity, creativity, and progress in psychological science? These questions can and should be studied empirically, and I present my predictions here. The productivity of individual researchers is likely to decline, although some changes (e.g., greater collaboration, data sharing) may mitigate this effect. The effects of these changes on creativity are likely to be mixed: Researchers will be less likely to pursue risky questions; more likely to use a broad range of methods, designs, and populations; and less free to define their own best practices and standards of evidence. Finally, the rate of scientific progress---the most important shared goal of scientists---is likely to increase as a result of these changes, although one's subjective experience of making progress will likely become rarer.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/TCJ2NL2D/Vazire - 2018 - Implications of the Credibility Revolution for Pro.pdf}
}

@article{vazire2022,
  title = {Where {{Are}} the {{Self-Correcting Mechanisms}} in {{Science}}?},
  author = {Vazire, Simine and Holcombe, Alex O.},
  year = {2022},
  month = jun,
  journal = {Review of General Psychology},
  volume = {26},
  number = {2},
  pages = {212--223},
  issn = {1089-2680, 1939-1552},
  doi = {10.1177/10892680211033912},
  urldate = {2023-06-20},
  abstract = {It is often said that science is self-correcting, but the replication crisis suggests that self-correction mechanisms have fallen short. How can we know whether a particular scientific field has effective self-correction mechanisms, that is, whether its findings are credible? The usual processes that supposedly provide mechanisms for scientific self-correction, such as journal-based peer review and institutional committees, have been inadequate. We describe more verifiable indicators of a field's commitment to self-correction. These fall under the broad headings of 1) transparency, which is already the subject of many reform efforts and 2) critical appraisal, which has received less attention and which we focus on here. Only by obtaining Observable Self-Correction Indicators (OSCIs) can we begin to evaluate the claim that ``science is self-correcting.'' We expect that the veracity of this claim varies across fields and subfields, and suggest that some fields, such as psychology and biomedicine, fall far short of an appropriate level of transparency and, especially, critical appraisal. Fields without robust, verifiable mechanisms for transparency and critical appraisal cannot reasonably be said to be self-correcting, and thus do not warrant the credibility often imputed to science as a whole.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/J3QTFQNW/Vazire_&_Holcombe_Where_are_the_self-correcting_mechanisms_June_2021.docx;/Users/anniewhamond/Zotero/storage/TY67TSDX/vazire-holcombe-2021-where-are-the-self-correcting-mechanisms-in-science.pdf}
}

@article{vazire2022a,
  title = {Credibility {{Beyond Replicability}}: {{Improving}} the {{Four Validities}} in {{Psychological Science}}},
  shorttitle = {Credibility {{Beyond Replicability}}},
  author = {Vazire, Simine and Schiavone, Sarah R. and Bottesini, Julia G.},
  year = {2022},
  month = apr,
  journal = {Current Directions in Psychological Science},
  volume = {31},
  number = {2},
  pages = {162--168},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/09637214211067779},
  urldate = {2023-06-20},
  abstract = {Psychological science's ``credibility revolution'' has produced an explosion of metascientific work on improving research practices. Although much attention has been paid to replicability (reducing false positives), improving credibility depends on addressing a wide range of problems afflicting psychological science, beyond simply making psychology research more replicable. Here we focus on the ``four validities'' and highlight recent developments---many of which have been led by early-career researchers---aimed at improving these four validities in psychology research. We propose that the credibility revolution in psychology, which has its roots in replicability, can be harnessed to improve psychology's validity more broadly.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/WIH9Y6BC/Vazire et al. - 2022 - Credibility Beyond Replicability Improving the Fo.pdf}
}

@article{vonelm2009,
  title = {The Role of Correspondence Sections in Post-Publication Peer Review: {{A}} Bibliometric Study of General and Internal Medicine Journals},
  shorttitle = {The Role of Correspondence Sections in Post-Publication Peer Review},
  author = {Von Elm, Erik and Wandel, Simon and J{\"u}ni, Peter},
  year = {2009},
  month = dec,
  journal = {Scientometrics},
  volume = {81},
  number = {3},
  pages = {747--755},
  issn = {0138-9130, 1588-2861},
  doi = {10.1007/s11192-009-2236-0},
  urldate = {2023-04-11},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/U2BJLCU8/Von Elm et al. - 2009 - The role of correspondence sections in post-public.pdf}
}

@article{wagenmakers2011,
  title = {Why Psychologists Must Change the Way They Analyze Their Data: {{The}} Case of Psi: {{Comment}} on {{Bem}} (2011).},
  shorttitle = {Why Psychologists Must Change the Way They Analyze Their Data},
  author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and Van Der Maas, Han L. J.},
  year = {2011},
  month = mar,
  journal = {Journal of Personality and Social Psychology},
  volume = {100},
  number = {3},
  pages = {426--432},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/a0022790},
  urldate = {2023-06-08},
  langid = {english},
  file = {/Users/anniewhamond/Documents/University/4th Year/Research Project/Papers/Read/Wagenmakers et al., 2011_response to Bem.pdf}
}

@article{whitely2020,
  title = {Antidepressant {{Prescribing}} and {{Suicide}}/{{Self-Harm}} by {{Young Australians}}: {{Regulatory Warnings}}, {{Contradictory Advice}}, and {{Long-Term Trends}}},
  shorttitle = {Antidepressant {{Prescribing}} and {{Suicide}}/{{Self-Harm}} by {{Young Australians}}},
  author = {Whitely, Martin and Raven, Melissa and Jureidini, Jon},
  year = {2020},
  month = jun,
  journal = {Frontiers in Psychiatry},
  volume = {11},
  pages = {478},
  issn = {1664-0640},
  doi = {10.3389/fpsyt.2020.00478},
  urldate = {2023-09-27},
  file = {/Users/anniewhamond/Zotero/storage/N6D8VQDS/Whitely et al. - 2020 - Antidepressant Prescribing and SuicideSelf-Harm b.pdf}
}

@article{yarkoni2022,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  year = {2022},
  journal = {Behavioral and Brain Sciences},
  volume = {45},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X20001685},
  urldate = {2024-05-02},
  abstract = {Abstract             Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned -- that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology -- the linear mixed model -- I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the ``random effect'' formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  file = {/Users/anniewhamond/Zotero/storage/9CG35W6T/Yarkoni - 2022 - The generalizability crisis.pdf}
}

@article{zotero-1847,
  title = {Yeo-{{Teh}}, {{N}}. {{S}}. {{L}}., \& {{Tang}}, {{B}}. {{L}}. (2021). {{An}} Alarming Retraction Rate for Scientific Publications on {{Coronavirus Disease}} 2019 ({{COVID-19}}). {{Accountability}} in Research, 28(1), 47-53.},
  doi = {10.1080/08989621.2020.1782203}
}
