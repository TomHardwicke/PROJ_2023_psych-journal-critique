---
title: "How do psychology journals handle post-publication critique? A cross-sectional study of policy and practice."
author: Whamond, A., Vazire, S., Clarke, B., Moodie, N., Schiavone, S., Thibault, R. T., & Hardwicke, T. E.
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  word_document: default
abstract: "Post-publication critique, such as letters-to-the-editor, can contribute
  to the validity and trustworthiness of scientific research. We conducted a cross-sectional
  and (b) prominent psychology journals. In 2023, an explicit submission option for
  post-publication critique was available at 23% (95% CI [16% to 32%]) of randomly
  sampled psychology journals (N = 100) and 38% of the most prominent psychology journals
  (N = 100). Journals sometimes imposed limits on the length and time allowed to submit
  critiques. We manually inspected two random samples of empirical articles published
  in 2020 (N = 101 articles per sample), estimating the prevalence of post-publication
  critique to be 0% (95% CI [0% to 3.7%]) in psychology journals generally and 1%
  (95% CI [0.2% to 5.4%]) in the most prominent psychology journals. The policy and
  practice of post-publication critique is seriously neglected in psychology journals.\n"
keywords: "post-publication critique, meta-research, journal policy, self-correction,
  peer review"
bibliography: references.bib
link-citations: true
csl: "CSL-apa.csl"
always_allow_html: true
urlcolor: blue
header-includes:
- \usepackage{float}
- \usepackage{pdflscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r}
#library(papaja) # article template
library(tidyverse)
library(dplyr)
library(kableExtra) # for tables
library(knitr) # for literate programming
library(tidylog) # for inline code feedback
library(here) # for finding files
library(binom) # for calculating CIs
library(citation) # for organizing citations
library(tinytex)
library(english) # for displaying code generated numbers as written 
```

```{r}
# Load processed data files
#'ProjCode', 'GitClone', 'PROJ_2023_psych-journal-critique',
d_articles <- read_csv(here('data', 'processed', 'd_articles.csv'), show_col_types = F) # article data 
d_journals <- read_csv(here('data', 'processed', 'd_journals.csv'), show_col_types = F) # journal data

```

```{r}
# Custom function to convert text to sentence case
sentence_case <- function(x) {
  s <- tolower(x)
  substr(s, 1, 1) <- toupper(substr(s, 1, 1))
  s
}
```

```{r}
# Create separate tibbles for Prominent and Randomly-selected journal samples

dj_prominent <- d_journals %>%
  filter(sample_id != "random") %>% # remove entries from the random sample
  filter(is_empirical == TRUE) # remove ineligible (non-empirical) journals

dj_random <- d_journals %>%
  filter(sample_id != "prominent") %>% # remove entries from the prominent sample
  filter(is_empirical == TRUE) # remove ineligible (non-empirical) journals
```

\newpage

# Research transparency statement

**Conflicts of interest:** All authors declare no conflicts of interest. **Funding:** This research was supported by Australian Research Council grant FT210100652. **Artificial intelligence:** No artificial intelligence assisted technologies were used in this research or the creation of this article. **Ethics:** Not applicable. **Preregistration:** The study protocol (aims, methods, and analyses) was preregistered [(https://osf.io/d6xf2)](https://osf.io/d6xf2) on April 19th 2023, prior to data collection which began on May 10th 2023. There were no major deviations from the preregistered protocol; minor deviations are reported in Supplementary Table A1. **Materials:** All study materials are publicly available [(https://osf.io/8k7m4/)](https://osf.io/8k7m4/). **Data:** All primary data are publicly available [(https://osf.io/8k7m4/)](https://osf.io/8k7m4/). **Analysis scripts:** All analysis scripts are publicly available [(https://osf.io/8k7m4/)](https://osf.io/8k7m4/). **Computational reproducibility:** The analysis code is available in a Code Ocean container [(https://codeocean.com/capsule/4794801/tree)](https://codeocean.com/capsule/4794801/tree) which re-creates the software environment in which the original analyses were performed. 

# Introduction

Critical discourse can contribute to the validity and trustworthiness of scientific research [@robertk.merton1973; @vazire2022]. Criticism can highlight errors, limitations, and alternative interpretations, helping research consumers to calibrate their beliefs in scientific claims. However, the proliferation of poor-quality research in psychology (and other fields) indicates that the dominant mode of critical discourse, conventional peer review, is not sufficient to maintain high standards in the scientific literature [@hardwicke2020; @nosek2022]. 

Scientific journals can support the ongoing critical scrutiny of research by offering avenues for post-publication critique, such as letters to the editor [@altman2002]. Journals’ obligation to facilitate post-publication critique is explicitly codified in the principles of the Committee on Publication Ethics (COPE), an organisation that most prominent journals are members of [@committeeonpublicationethics2021; @hardwicke2022]. However, prior research indicates that many (37%) of the most prominent academic journals across a broad range of scientific disciplines do not explicitly allow submissions of post-publication critique [@hardwicke2022]. Moreover, journals that do allow post-publication critiques in principle rarely publish them in practice, and often impose restrictive limits on length and time-to-submit. The present research closely follows the methods used in @hardwicke2022 but extends this prior work by providing a focused evaluation of how psychology journals in particular handle post-publication critique (Hardwicke et al. examined 15 journals in a combined psychiatry and psychology category, along with 315 journals from other disciplines).

For the purposes of the present study, we operationally defined post-publication critique as any journal-based avenue for sharing peer-initiated critical discourse related to specific research articles previously published in the same journal (see Supplementary Information B for a complete operational definition). A prototypical example of post-publication critique is a letter to the editor, though there are other variations, such as commentary articles and website comments. 

Our study was conducted in two parts. The first part aimed to describe policies related to post-publication critique at: (a) all psychology journals publishing empirical research (via a random sample of 100 journals), and (b) the 100 most prominent^[There is no agreed upon or objective measure of journal prominence. To avoid an entirely subjective and arbitrary definition, we used Journal Impact Factor as a proxy for journal prominence. Journal Impact Factor, like all journal metrics, has limitations (especially when used as a proxy for quality, which is not what we are doing here), but using this method yields a collection of journals that we believe has good face validity in terms of prominence.] psychology journals publishing empirical research. Specifically, we identified whether options for submitting critiques (e.g., letters to the editor) are available, and whether limits are imposed in terms of length, time-to-submit since publication of the target article, number of references, and whether critiques were sent for independent peer review. The second part of our study aimed to estimate the prevalence of post-publication critiques published in: (a) all psychology journals (via a random sample of 100 empirical psychology articles), and (b) the 100 most prominent psychology journals (via a random sample of 100 empirical articles published in these journals).

# Part One: Post-Publication Critique Policies

## Method

The methods and analysis plan were preregistered [(https://osf.io/d6xf2)](https://osf.io/d6xf2). All deviations from the preregistered protocol were considered minor and are detailed in Supplementary Information A and, where relevant, mentioned below.

### Design

This investigation had a cross-sectional design. All measured variables are described in detail in Supplementary Table C1. In brief, we extracted and described journal policies related to post-publication critique (according to our operational definition, Supplementary Information B) provided on journal websites in 2023. We recorded the name and description of any options for post-publication critique offered by each journal (e.g., article types such as ‘letters’ or ‘commentaries’, or web comments) or any explicit statement that the journal does not accept post-publication critique; limits imposed on post-publication critique in terms of length (e.g., number of words), time-to-submit (e.g., weeks since publication of the target article) and/ or number of references; and whether it was stated that post-publication critiques would be sent for independent external peer review (i.e., reviews solicited from individuals who were not members of the editorial team or authors of the target article). We recorded qualitative as well as quantitative limits; for example, a length limit might be stated as “500 words” or “short”. We also identified whether journals were members of COPE, an organisation which outlines several core publishing practices, including the importance of facilitating post-publication critique [@committeeonpublicationethics2021].

### Sample

*Target populations*

The target populations of the two samples of journals were: (a) all psychology journals publishing empirical research in 2023; and (b) prominent journals in psychology publishing empirical research in 2023. We were only interested in legitimate (non-predatory), peer-reviewed journals.

*Sample sizes*

The sample consisted of 100 randomly selected journals and 100 prominent journals. Justification for these sample sizes was based on a precision analysis (details provided in Supplementary Information E). Eleven journals appeared in both samples and were not replaced in order to maintain the random sampling scheme.

*Preparing samples (inclusion/exclusion criteria)*

We only included journals that were in the Web of Science Core Collection, meaning they met 24 basic quality criteria, such as having a peer-review policy and publishing scholarly content (for details see https://perma.cc/C5J3-V7C4). As of 15th March 2023, there were 867 unique journals included in this database that were classified as belonging to a subject category related to ‘psychology’. Of these 867 journals, 115 were classified by Web of Science as non-English language and excluded, leaving 752 journals remaining. 
For the random sample of psychology journals, a list of all included 752 psychology journals was randomly shuffled using the R function ‘sample’ and we selected the first 600 on the shuffled list (to allow for exclusions). For the prominent journals sample, we sorted journals in order of their 2021 Clarivate Journal Impact Factors (https://jcr.clarivate.com/) and selected the first 600 journals (to allow for exclusions). The list of these journals is available at https://osf.io/k6yx5. For both samples, journals were then evaluated sequentially from the top of their respective list until the target of 100 eligible journals in each sample was reached. If a journal needed to be excluded after manual inspection, it was replaced with the next available journal in its list, thus maintaining the respective sampling schemes.
We excluded an additional 6 journals from the randomly-selected journal list and 15 from the prominent list because they did not publish empirical research. This determination involved some degree of subjective judgement and was primarily based on each journal’s scope as defined on its website. If the journal’s description of its scope was ambiguous, we searched the journal's archive for examples of empirical articles to inform our determination. This represented a slight deviation from the preregistered protocol (for details see Supplementary Information A).

### Procedure

*Piloting and training*

The data extraction procedures were piloted with 10  journals that were not included in the sample. All investigators were trained and familiarised with the data extraction process and practised with positive and negative training examples.

*Collating journal policy information*

One investigator (AW or NM) identified, downloaded, and archived the ‘scope’ and ‘article types’ sections (or equivalent) provided on each journal’s website [(https://osf.io/8k7m4/)](https://osf.io/8k7m4/). This information was used to determine whether the journal published empirical articles and, if so, whether post-publication critique article types were accepted by the journal. If these sections could not be found, a second investigator (TEH) double-checked.

*Data extraction*

The policy information (outlined in Supplementary Information C using a Goole Form [(https://osf.io/d6uzj)](https://osf.io/d6uzj) was extracted and classified for each journal by two investigators. Primary coding was conducted by AW (n = 183) and NM (n = 37). Secondary coding was conducted by RTT (n = 82), SV (n = 80), and TEH (n = 41).  Disagreements between primary and secondary coders, usually arising due to unclear descriptions of article types, were resolved by TEH. This was necessary for 40 journals (for details see Supplementary Information H).
For coding whether journals provided an option for post-publication critique, we first examined the journal website’s description of the journal’s article types (typically found in journals’ submission guidelines or instructions to authors). If a journal’s description of its article types was not provided or unclear, we searched the journal's archive to see if any examples of articles meeting our operational definition of post-publication critique (Supplementary Information B) had been published within the last five years (i.e., since the beginning of 2019). Note that if a journal had an article type that sounded like it might be used for post-publication critique (e.g., "Commentary"), that was not sufficient for a positive classification; the description of the article type, or examples of that article type, also had to meet our operational definition of post-publication critique (see Supplementary Information B).

*Data harmonization*

Journals used various naming conventions for similar types of post-publication critique and various units (e.g., characters, words or pages for length limits) to specify limits. To address this, we harmonised the various names used for critique formats to four types (letters [to the editor], commentaries, web comments, and other) and transformed length limit units to words and time-to-submit limit units to weeks (for details see Supplementary Information G). 

## Results

### Journal characteristics

```{r}
# Calculate JIF median and IQR for random journals
randomJIF <- dj_random %>% 
  filter(ppc_type == "A") %>%
  filter(!is.na(jif)) %>%
  summarise(Median = round(median(jif),2),
            Q1 = round(quantile(jif, .25),2),
            Q3 = round(quantile(jif, .75),2),
            IQR = round(IQR(jif),2)
  )
```

```{r}
# Calculate JIF median and IQR for prominent journals
prominentJIF <- dj_prominent %>% 
  filter(ppc_type == "A") %>%      # Keep only single entry for each journal
  summarise(Median = round(median(jif),2), 
            Q1 = round(quantile(jif, .25),2),
            Q3 = round(quantile(jif, .75),2),
            IQR = round(IQR(jif),2)
  )
```

```{r}
# Count randomly selected journals listed as COPE members
randomCOPE <- dj_random %>%
  filter(ppc_type == 'A') %>%
  filter(cope == TRUE) %>%
  summarise(count = n())
```

```{r}
# Count prominent journals listed as COPE members 
prominentCOPE <- dj_prominent %>%
  filter(ppc_type == 'A') %>%
  filter(cope == TRUE) %>%
  summarise(count = n())
```

The median 2021 Journal Impact Factor was `r randomJIF$randMedian_jif` (interquartile range = `r randomJIF$Q1` - `r randomJIF$Q3`) for the randomly sampled journals (NB. 25 of the randomly selected journals did not have Journal Impact Factors) and `r prominentJIF$Median` (interquartile range = `r prominentJIF$Q1` - `r prominentJIF$Q3`) for the prominent journals. `r randomCOPE` of the `r dj_random %>% filter(ppc_type == 'A') %>% nrow()` randomly sampled journals, and `r prominentCOPE` of the `r dj_prominent %>% filter(ppc_type == 'A') %>% nrow()` prominent journals, were members of COPE. Journal characteristics stratified by psychology sub-field are available in Supplementary Information I.

### How many journals offer post-publication critique?

```{r}
# Inspect how frequently PPC policies were stated for randomly selected journals 
randomPPC <- dj_random %>%
  filter(ppc_type == 'A') %>%
  group_by(has_ppc) %>%
  summarise(count = n()) %>%
  ungroup()
```

```{r}
# Calculate confidence interval for explicitly stated PPC policies among randomly selected journals
randomPPCyes <- dj_random %>%
  filter(ppc_type == 'A', !is.na(ppc_name)) %>%
  summarise(count = n())

randomPPCno <- dj_random %>%
  filter(ppc_type == 'A', is.na(ppc_name)) %>%
  summarise(count = n())

# Sepcify sample size and confidence interval of 95%
sample_size <- 100
confidence_level <- 0.95

# CIs for 'Yes Explicit'
randomPPC_CI <- binom.confint(randomPPCyes, sample_size, method = "wilson", conf.level = confidence_level)

# CIs for 'No' (both explicit and Implicit)
randomPPC_CI_neg <- binom.confint(randomPPCno, sample_size, method = "wilson", conf.level = confidence_level)
```

```{r}
# Return all rows with ppc_name values for ppc_type B and C indicating > 1 PPC type
random_multiPPC <- dj_random %>%
  filter((ppc_type == "B" | ppc_type == "C") & (!is.na(ppc_name))) 
```

```{r}
# Inspect the COPE status of randomly selected journals with and without PPC policies
randomCOPE <- dj_random %>%
 filter(ppc_type == "A") %>%
 mutate(has_ppc = case_when(
    has_ppc == "NO EXPLICIT" ~ "NO",
    has_ppc == "NO IMPLICIT" ~ "NO",
    TRUE ~ has_ppc
 )) %>%
 group_by(has_ppc) %>%
 summarise(
    total_count = n(),
    COPE_count = sum(cope == TRUE)
 ) %>%
 mutate(COPEpercent = round((COPE_count / total_count) * 100)) %>%
 ungroup()
```

A full list of journals and their post-publication critique options and limits is provided in Supplementary Information J (raw data available at [https://osf.io/8k7m4/](https://osf.io/8k7m4/)). Among the 100 randomly-selected journals, `r randomPPC$count[randomPPC$has_ppc == "YES"]` (`r randomPPC$count[randomPPC$has_ppc == "YES"]`%, 95% CI [`r round(randomPPC_CI[1,5]*100)`%, `r round(randomPPC_CI[1,6]*100)`%]) explicitly stated that they allowed submissions of post-publication critique. Of these `r randomPPC$count[randomPPC$has_ppc == "YES"]` journals, `r print(as.character(english(nrow(random_multiPPC))))` had two types of critique formats and the other `r (randomPPC$count[randomPPC$has_ppc == "YES"] - nrow(random_multiPPC))` each had one critique format. `r print(as.character(english(randomPPC$count[randomPPC$has_ppc == "NO EXPLICIT"])))` journal (*Clinical Psychology: Science and Practice*, also included in the prominent journals sample) stated explicitly that they did not accept post-publication critique. `r dj_random %>% filter(ppc_name == "Commentary") %>% nrow()` journals offered commentaries, `r dj_random %>% filter(ppc_name == "Letters") %>% nrow()` journals offered letters, `r dj_random %>% filter(ppc_name == "Web comments") %>% nrow()` journals offered web comments, and `r print(as.character(english(nrow(dj_random %>% filter(ppc_name == "Other")))))` journal (*Cortex*) offered a format called 'verification reports' which we classified as ‘other’. Of the `r randomPPC$count[randomPPC$has_ppc == "YES"]` journals that explicitly accepted post-publication critique, `r randomCOPE$COPE_count[randomCOPE$has_ppc == "YES"]` (`r randomCOPE$COPEpercent[randomCOPE$has_ppc == "YES"]`%) were COPE members. Of the `r randomCOPE$COPE_count[randomCOPE$has_ppc == "NO"]` journals that did not have a policy related to post-publication critique, `r randomCOPE$COPE_count[randomCOPE$has_ppc == "NO"]` (`r randomCOPE$COPEpercent[randomCOPE$has_ppc == "NO"]`%) were COPE members. 

```{r}
# Inspect how frequently PPC policies were stated for prominent journals 
prominentPPC <- dj_prominent %>%
  filter(ppc_type == 'A') %>%
  group_by(has_ppc) %>%
  summarise(count = n()) %>%
  ungroup()
```

```{r}
# Get names for prominent journals that offered more than one type of PPC
prom_multiPPC <- dj_prominent %>%
  filter((ppc_type == "B" | ppc_type == "C") & (!is.na(ppc_name))) 
```

```{r}
# Inspect the COPE status of prominent journals with and without PPC policies
promCOPE <- dj_prominent %>%
 filter(ppc_type == "A") %>%
 mutate(has_ppc = case_when(
    has_ppc == "NO EXPLICIT" ~ "NO",
    has_ppc == "NO IMPLICIT" ~ "NO",
    TRUE ~ has_ppc
 )) %>%
 group_by(has_ppc) %>%
 summarise(
    total_count = n(),
    COPE_count = sum(cope == TRUE)
 ) %>%
 mutate(COPEpercent = round((COPE_count / total_count) * 100)) %>%
 ungroup()
```

Among the 100 prominent journals, `r prominentPPC$count[prominentPPC$has_ppc == "YES"]` (`r prominentPPC$count[prominentPPC$has_ppc == "YES"]`%) explicitly stated that they allowed submissions of post-publication critique^[Confidence intervals are not included for the prominent psychology journals as this sample represents our entire population of interest, negating the need for inferential statistical calculations.]. Of these `r prominentPPC$count[prominentPPC$has_ppc == "YES"]` journals, `r print(as.character(english(nrow(prom_multiPPC))))` had two types of critique format and the other `r (prominentPPC$count[prominentPPC$has_ppc == "YES"] - nrow(prom_multiPPC))` journals had one critique format. `r print(as.character(english(prominentPPC$count[prominentPPC$has_ppc == "NO EXPLICIT"])))` journals (*Clinical Psychology: Science and Practice* and *Psychological Bulletin*) stated explicitly that they did not accept post-publication critiques. No reason was provided. `r dj_prominent %>% filter(ppc_name == "Commentary") %>% nrow()` journals offered commentaries, `r dj_prominent %>% filter(ppc_name == "Letters") %>% nrow()` journals offered letters, and `r dj_prominent %>% filter(ppc_name == "Web comments") %>% nrow()` journals offered web comments. Of the `r prominentPPC$count[prominentPPC$has_ppc == "YES"]` journals that explicitly accepted post-publication critique, `r promCOPE$COPE_count[promCOPE$has_ppc == "YES"]` (`r promCOPE$COPEpercent[promCOPE$has_ppc == "YES"]`%) were COPE members. Of the `r promCOPE$COPE_count[promCOPE$has_ppc == "NO"]` journals that did not have a policy related to post-publication critique, `r promCOPE$COPE_count[promCOPE$has_ppc == "NO"]` (`r promCOPE$COPEpercent[promCOPE$has_ppc == "NO"]`%) were COPE members.

### What limits did journals place on post-publication critique?

```{r}
# Find the most restrictive length limit of any journal
minLength <- d_journals %>%
 select(c(sample_id, journal_name, ppc_name, ppc_length)) %>%
 filter(!is.na(ppc_name),
         ppc_length != "NOT STATED" & ppc_length != "Qualitative") %>%
 mutate(quantLength = as.numeric(ppc_length)) %>%
 slice_min(quantLength, n = 1) %>% # Select the row with the minimum quantLength
 ungroup()

# Find the most restrictive time limit of any journal
minTime <- d_journals %>%
 select(c(sample_id, journal_name, ppc_name, ppc_time)) %>%
 filter(!is.na(ppc_name),
         ppc_time != "NOT STATED" & ppc_time != "Qualitative") %>%
 mutate(quantTime = as.numeric(ppc_time)) %>%
 slice_min(quantTime, n = 1) %>% # Select the row with the minimum quantTime
 ungroup()
```

Table 1 (randomly selected journals) and Table 2 (prominent journals) show the limits journals imposed on post-publication critique in terms of length, time-to-submit, and number of references. The tables also show whether a peer review policy for critiques was stated. Length limits were stated for most critique formats, especially those in prominent journals. Length limits were usually stated quantitatively (e.g., “1,000 words”). A few formats had qualitative limits (e.g., “brief”). Letters had tighter length limits than commentaries on average. The most restrictive length limit was `r minLength$quantLength` words at the *Journal of Adolescent Health*. Time-to-submit limits were not stated for most critique formats; they were somewhat more common in prominent journals. Time-to-submit limits, when present, were often stated qualitatively (e.g., “brief”) and sometimes quantitatively (e.g., “3 months”). The most restrictive time-to-submit limit was `r minTime$quantTime` weeks at the *Journal of Applied Psychology*. For most critique formats, journals did not state whether they would be sent for independent peer review or not. For a full list of stated limits at each journal, see Supplementary Information J.

\newpage
\begin{landscape}

\textbf{Table 1.} Post-publication critique types identified in randomly selected psychology journals and their length, time-to-submit, and reference limits. 
```{r}
# Check length limits placed on PPC options in randomly selected journals

# Summarise length statistics for All Types
randomLengthAll <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_length)) %>%
 mutate(quantLength = case_when(
    ppc_length == "NOT STATED" | ppc_length == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_length) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_length == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_length == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianLength = median(quantLength, na.rm = TRUE),
    Q1 = round(quantile(quantLength, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantLength, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) 

# Put results into desired table format
randomLengthAll_1 <- randomLengthAll %>% 
  mutate('Post-publication critique type' = "All types") %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianLength, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

``` {r}
# Summarise length Statistics for each PPC type
randomLengthType <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_length)) %>%
 mutate(quantLength = case_when(
    ppc_length == "NOT STATED" | ppc_length == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_length) # create a column that contains only quantitative limits
  )) %>%
 group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_length == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_length == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianLength = round(median(quantLength, na.rm = TRUE)),
    Q1 = round(quantile(quantLength, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantLength, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1) 
 ) %>%
  ungroup()

# Put results into desired table format
randomLengthType_1 <- randomLengthType %>% 
  mutate('Post-publication critique type' = ppc_name) %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianLength, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all time limits into single table
randomLength_combo <- bind_rows(
  randomLengthType_1,
  randomLengthAll_1
)
```

```{r}
# Check time-to-submit limits placed on PPC options in prominent journals

# Summarise time-to-submit statistics for All Types
randomTimeAll <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_time)) %>%
 mutate(quantTime = case_when(
    ppc_time == "NOT STATED" | ppc_time == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_time) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_time == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_time == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianTime = round(median(quantTime, na.rm = TRUE)),
    Q1 = round(quantile(quantTime, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantTime, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1) 
 ) 

# Put results into desired table format
randomTimeAll_1 <- randomTimeAll %>% 
  mutate('Post-publication critique type' = "All Types") %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianTime, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

``` {r}
# Summarise time-to-submit Statistics for each PPC type
randomTimeType <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_time)) %>%
 mutate(quantTime = case_when(
    ppc_time == "NOT STATED" | ppc_time == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_time) # create a column that contains only quantitative limits
  )) %>%
  group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_time == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_time == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianTime = round(median(quantTime, na.rm = TRUE)),
    Q1 = round(quantile(quantTime, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantTime, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) %>%
  ungroup()

# Put results into desired table format
randomTimeType_1 <- randomTimeType %>% 
  mutate('Post-publication critique type' = ppc_name) %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianTime, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all time limits into single table
randomTime_combo <- bind_rows(
  randomTimeType_1,
  randomTimeAll_1
)
```

```{r}
# Check reference limits placed on PPC options in prominent journals

# Summarise reference statistics for All Types
randomRefAll <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_ref)) %>%
 mutate(quantRef = case_when(
    ppc_ref == "NOT STATED" | ppc_ref == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_ref) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_ref == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_ref == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianRef = median(quantRef, na.rm = TRUE),
    Q1 = round(quantile(quantRef, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantRef, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 )

# Put results into desired table format
randomRefAll_1 <- randomRefAll %>% 
  mutate('Post-publication critique type' = "All types") %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianRef, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Summarise reference statistics by type
randomRefType <- dj_random %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_ref)) %>%
 mutate(quantRef = case_when(
    ppc_ref == "NOT STATED" | ppc_ref == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_ref) # create a column that contains only quantitative limits
  )) %>%
  group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_ref == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_ref == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianRef = median(quantRef, na.rm = TRUE),
    Q1 = round(quantile(quantRef, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantRef, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) %>%
  ungroup()

# Put results into desired table format
randomRefType_1 <- randomRefType %>% 
  mutate('Post-publication critique type' = ppc_name) %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianRef, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all peer review into single table
randomRef_combo <- bind_rows(
  randomRefType_1,
  randomRefAll_1
)
```

```{r}
# Check how many PPC options are sent for external peer review in prominent journals

# Peer review statistics for All Types
randomReviewAll <- dj_random %>%
  select(c(journal_name, ppc_name, ppc_review)) %>%
  filter(!is.na(ppc_name)) %>%
  summarise(anyReview = n() - sum(ppc_review == "NOT STATED"),
            yesReview = anyReview - sum(ppc_review == "NO"),
            yesPercent = round((yesReview / n()) * 100)   # Percentage reviewed from All Types 
  )

# Put results into desired table format
randomReviewAll_1 <- randomReviewAll %>%
  mutate('Post-publication critique type' = "All types") %>%
  mutate('YES n (%)' = paste(yesReview, " (", yesPercent, ")", sep = "")) %>%
  select('Post-publication critique type', 'YES n (%)')
```

```{r}
# Peer review statistics by types
randomReviewType <- dj_random %>%
  select(c(journal_name, ppc_name, ppc_review)) %>%
  filter(!is.na(ppc_name)) %>%
  group_by(ppc_name) %>%
  summarise(anyReview = n() - sum(ppc_review == "NOT STATED"),
            yesReview = anyReview - sum(ppc_review == "NO"),
            yesPercent = round((yesReview / n()) * 100)
  ) %>%
  ungroup()

# Put results into desired table format
randomReviewType_1 <- randomReviewType %>%
  mutate('Post-publication critique type' = ppc_name) %>%
  mutate('YES n (%)' = paste(yesReview, " (", yesPercent, ")", sep = "")) %>%
  select('Post-publication critique type', 'YES n (%)')
```

```{r}
# Combine all peer review into single table
randomReview_combo <- bind_rows(
  randomReviewType_1,
  randomReviewAll_1
)
```

``` {r}
# Give each table a separate header for results
randomLengthTable <- randomLength_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Length limits" = 3))

randomTimeTable <- randomTime_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Time limits" = 3))

randomRefTable <- randomRef_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Reference limits" = 3))

randomReviewTable <- randomReview_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "YES n (%)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Peer review" = 1))
```

```{r}
# Combine the random sample limitations data frames
combined_df <- bind_cols(
  randomLength_combo %>% select(-`Post-publication critique type`),
  randomTime_combo %>% select(-`Post-publication critique type`),
  randomRef_combo %>% select(-`Post-publication critique type`),
  randomReview_combo %>% select(-`Post-publication critique type`)
)

# Add the 'Post-publication critique type' column back to the combined data frame
combined_df <- bind_cols(randomLength_combo %>% select(`Post-publication critique type`),
                         combined_df)

# Create the table with kableExtra
randomLimits_Table <- combined_df %>%
  kable("latex", col.names = c("Post-publication critique type", 
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "'Yes' n (%)")
        ) %>%
  column_spec(1, width = "2.6cm") %>% 
  column_spec(c(2, 5, 8), width = "1.6cm") %>% 
  column_spec(c(3, 6, 9), width = "2cm") %>% 
  column_spec(c(4, 7, 10), width = "1.5cm") %>% 
  column_spec(c(11), width = "1.2cm") %>% 
  kable_styling("scale_down", latex_options = "H") %>%
  add_header_above(c(" " = 1, 
                     "Length limits" = 3, 
                     "Time limits" = 3, 
                     "Reference limits" = 3, 
                     "Peer review" = 1)
                   ) 
```
`r randomLimits_Table`\textit{Note.} This table also indicates whether the journal states that critiques will be sent for independent external peer review. IQR = Interquartile Range. Note that n’s refer to number of critique formats, not number of journals (some journals had more than one format of post publication critique). All results rounded to the nearest integer.

\textbf{Table 2.} Post-publication critique types identified in prominent psychology journals and their length, time-to-submit, and reference limits. 

```{r}
# Check length limits placed on PPC options in prominent journals

# Summarise length statistics for All Types
prominentLengthAll <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_length)) %>%
 mutate(quantLength = case_when(
    ppc_length == "NOT STATED" | ppc_length == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_length) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_length == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_length == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianLength = median(quantLength, na.rm = TRUE),
    Q1 = round(quantile(quantLength, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantLength, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) 

# Put results into desired table format
prominentLengthAll_1 <- prominentLengthAll %>% 
  mutate('Post-publication critique type' = "All types") %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianLength, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

``` {r}
# Summarise length Statistics for each PPC type
prominentLengthType <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_length)) %>%
 mutate(quantLength = case_when(
    ppc_length == "NOT STATED" | ppc_length == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_length) # create a column that contains only quantitative limits
  )) %>%
 group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_length == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_length == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianLength = median(quantLength, na.rm = TRUE),
    Q1 = round(quantile(quantLength, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantLength, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) %>%
  ungroup()

# Put results into desired table format
prominentLengthType_1 <- prominentLengthType %>% 
  mutate('Post-publication critique type' = ppc_name) %>% 
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>% 
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>% 
  mutate('median (IQR)' = paste(medianLength, " (", Q1, " - ", Q3, ")", sep = "")) %>% 
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all peer review into single table
prominentLength_combo <- bind_rows(
  prominentLengthType_1,
  prominentLengthAll_1
)
```

```{r}
# Check time-to-submit limits placed on PPC options in prominent journals

# Summarise time-to-submit statistics for All Types
prominentTimeAll <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_time)) %>%
 mutate(quantTime = case_when(
    ppc_time == "NOT STATED" | ppc_time == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_time) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_time == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_time == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianTime = round(median(quantTime, na.rm = TRUE)),
    Q1 = round(quantile(quantTime, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantTime, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1) 
 ) 

# Put results into desired table format
prominentTimeAll_1 <- prominentTimeAll %>%
  mutate('Post-publication critique type' = "All types") %>%
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>%
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>%
  mutate('median (IQR)' = paste(medianTime, " (", Q1, " - ", Q3, ")", sep = "")) %>%
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

``` {r}
# Summarise time-to-submit Statistics for each PPC type
prominentTimeType <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_time)) %>%
 mutate(quantTime = case_when(
    ppc_time == "NOT STATED" | ppc_time == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_time) # create a column that contains only quantitative limits
  )) %>%
  group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_time == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_time == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianTime = round(median(quantTime, na.rm = TRUE)),
    Q1 = round(quantile(quantTime, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantTime, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) %>%
  ungroup()

# Put results into desired table format
prominentTimeType_1 <- prominentTimeType %>%
  mutate('Post-publication critique type' = ppc_name) %>%
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>%
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>%
  mutate('median (IQR)' = paste(medianTime, " (", Q1, " - ", Q3, ")", sep = "")) %>%
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all peer review into single table
prominentTime_combo <- bind_rows(
  prominentTimeType_1,
  prominentTimeAll_1
)
```

```{r}
# Check reference limits placed on PPC options in prominent journals

# Summarise reference statistics for All Types
prominentRefAll <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_ref)) %>%
 mutate(quantRef = case_when(
    ppc_ref == "NOT STATED" | ppc_ref == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_ref) # create a column that contains only quantitative limits
  )) %>%
 summarise(
    anyLimit = n() - sum(ppc_ref == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_ref == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianRef = median(quantRef, na.rm = TRUE),
    Q1 = round(quantile(quantRef, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantRef, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) 

# Put results into desired table format
prominentRefAll_1 <- prominentRefAll %>%
  mutate('Post-publication critique type' = "All types") %>%
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>%
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>%
  mutate('median (IQR)' = paste(medianRef, " (", Q1, " - ", Q3, ")", sep = "")) %>%
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Summarise reference statistics by type
prominentRefType <- dj_prominent %>%
 filter(!is.na(ppc_name)) %>% 
 select(c(journal_name, ppc_name, ppc_ref)) %>%
 mutate(quantRef = case_when(
    ppc_ref == "NOT STATED" | ppc_ref == "Qualitative" ~ NA_real_,
    TRUE ~ as.numeric(ppc_ref) # create a column that contains only quantitative limits
  )) %>%
  group_by(ppc_name) %>%
 summarise(
    anyLimit = n() - sum(ppc_ref == "NOT STATED"),
    anyPercent = round((anyLimit / n()) * 100),
    quantLimit = anyLimit - sum(ppc_ref == "Qualitative"),
    quantPercent = round((quantLimit / n()) * 100),
    medianRef = median(quantRef, na.rm = TRUE),
    Q1 = round(quantile(quantRef, probs = 0.25, na.rm = TRUE)),
    Q3 = round(quantile(quantRef, probs = 0.75, na.rm = TRUE)),
    IQR = round(Q3 - Q1)
 ) %>%
  ungroup()

# Put results into desired table format
prominentRefType_1 <- prominentRefType %>%
  mutate('Post-publication critique type' = ppc_name) %>%
  mutate('any n (%)' = paste(anyLimit, " (", anyPercent, ")", sep = "")) %>%
  mutate('quantitative n (%)' = paste(quantLimit, " (", quantPercent, ")", sep = "")) %>%
  mutate('median (IQR)' = paste(medianRef, " (", Q1, " - ", Q3, ")", sep = "")) %>%
  select('Post-publication critique type', 'any n (%)', 'quantitative n (%)', 'median (IQR)')
```

```{r}
# Combine all peer review into single table
prominentRef_combo <- bind_rows(
  prominentRefType_1,
  prominentRefAll_1
)
```

```{r}
# Check how many PPC options are sent for external peer review in prominent journals

# Peer review statistics for All Types
prominentReviewAll <- dj_prominent %>%
  select(c(journal_name, ppc_name, ppc_review)) %>%
  filter(!is.na(ppc_name)) %>%
  summarise(anyReview = n() - sum(ppc_review == "NOT STATED"),
            yesReview = anyReview - sum(ppc_review == "NO"),
            yesPercent = round((yesReview / n()) * 100)   # Percentage reviewed from All Types 
  )

# Put results into desired table format
prominentReviewAll_1 <- prominentReviewAll %>%
  mutate('Post-publication critique type' = "All types") %>%
  mutate('YES n (%)' = paste(yesReview, " (", yesPercent, ")", sep = "")) %>%
  select('Post-publication critique type', 'YES n (%)')
```

```{r}
# Peer review statistics by types
prominentReviewType <- dj_prominent %>%
  select(c(journal_name, ppc_name, ppc_review)) %>%
  filter(!is.na(ppc_name)) %>%
  group_by(ppc_name) %>%
  summarise(anyReview = n() - sum(ppc_review == "NOT STATED"),
            yesReview = anyReview - sum(ppc_review == "NO"),
            yesPercent = round((yesReview / n()) * 100)
  ) %>%
  ungroup()

# Put results into desired table format
prominentReviewType_1 <- prominentReviewType %>%
  mutate('Post-publication critique type' = ppc_name) %>%
  mutate('YES n (%)' = paste(yesReview, " (", yesPercent, ")", sep = "")) %>%
  select('Post-publication critique type', 'YES n (%)')
```

```{r}
# Combine all peer review into single table
prominentReview_combo <- bind_rows(
  prominentReviewType_1,
  prominentReviewAll_1
)
```

``` {r}
# Give each table a separate header for results
prominentLengthTable <- prominentLength_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Length limits" = 3))

prominentTimeTable <- prominentTime_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Time limits" = 3))

prominentRefTable <- prominentRef_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "any n (%)", 
                              "quantitative n (%)", 
                              "median (IQR)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Reference limits" = 3))

prominentReviewTable <- prominentReview_combo %>%
  kable("html", col.names = c("Post-publication critique type", 
                              "YES n (%)")) %>%
  kable_styling(full_width = F) %>%
  add_header_above(c(" " = 1, "Peer review" = 1))

```

```{r}
# Combine the prominent limitations data frames
combined_df <- bind_cols(
  prominentLength_combo %>% select(-`Post-publication critique type`),
  prominentTime_combo %>% select(-`Post-publication critique type`),
  prominentRef_combo %>% select(-`Post-publication critique type`),
  prominentReview_combo %>% select(-`Post-publication critique type`)
)

# Add the 'Post-publication critique type' column back to the combined data frame
combined_df <- bind_cols(prominentLength_combo %>% select(`Post-publication critique type`), combined_df)

# Create the table with kableExtra
prominentLimits_Table <- combined_df %>%
  kable("latex", col.names = c("Post-publication critique type", 
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "any n (%)", "quantitative n (%)", "Median (IQR)",
                      "'Yes' n (%)") 
        ) %>%
 column_spec(1, width = "2.6cm") %>% 
  column_spec(c(2, 5, 8), width = "1.6cm") %>% 
  column_spec(c(3, 6, 9), width = "2cm") %>% 
  column_spec(c(4, 7, 10), width = "1.5cm") %>% 
  column_spec(c(11), width = "1.2cm") %>% 
  kable_styling("basic", latex_options = "H") %>%
  add_header_above(c(" " = 1, 
                     "Length limits" = 3, 
                     "Time limits" = 3, 
                     "Reference limits" = 3, 
                     "Peer review" = 1)
                   ) 
```
`r prominentLimits_Table`\textit{Note.} This table also indicates whether the journal states that the critiques will be sent for independent external peer review. IQR = Interquartile Range. Note that n’s refer to number of critique formats, not number of journals (some journals had more than one format of post publication critique). All results rounded to the nearest integer. 

\end{landscape}
\newpage

# Part Two: Post-Publication Critique Prevalence

## Method

The methods and analysis plan were preregistered [(https://osf.io/d6xf2)](https://osf.io/d6xf2). All deviations from the preregistered protocol were considered minor and are detailed in Supplementary Information A and, where relevant, mentioned below.

### Design

This investigation had an cross-sectional design. All measured variables are described in detail in Supplementary Information C2. In brief, we estimated the prevalence of post-publication critique among empirical articles published in 2020 using two methods: (a) we identified whether sampled articles contained a link to a post-publication critique of the article (“Linked Method”); and (b) we identified whether sampled articles were themselves examples of post-publication critique (“Instance Method”)^[In Hardwicke et al. (2022), the Linked Method and Instance Method of prevalence estimation were called the ‘primary’ and ‘secondary’ method respectively. We changed the terminology because we do not want to imply that one method is necessarily superior to the other.]. These two methods of estimating prevalence have complementary strengths and weaknesses; for more details, see Supplementary Information D.

### Sample

*Target populations*

The target populations were (a) all empirical articles published in 2020 in all psychology journals; and (b) all empirical articles published in 2020 in the 100 most prominent psychology journals identified in Part One. We were only interested in articles published in legitimate (non-predatory), peer-reviewed journals.

*Sample sizes*

We examined 101 randomly selected empirical articles published in psychology journals and 101 randomly selected empirical articles published in prominent psychology journals (our target sample size was 100 articles in each sample, but, deviating from our preregistration, we accidently examined one additional article in each sample, see Supplementary Information A for details). Justification for these sample sizes is provided in Supplementary Information E. 

*Preparing Samples (inclusion/exclusion criteria)*

For the random sample of articles published in psychology journals, we used the Web of Science Core Collection to download bibliographic classified with a Web of Science document type of ‘Article’, ‘Discussion’, ‘Editorial material’, or ‘Letter’ published in 2020 and classified as belonging to a psychology research area. The search returned 61,735 records (for exact search string, see Supplementary Information F).

For the random sample of articles published in the most prominent psychology journals, we used the Web of Science Core Collection to download bibliographic records classified with a Web of Science document type of ‘Article’, ‘Discussion’, ‘Editorial material’, or ‘Letter’ published in 2020 in the 100 most prominent psychology journals (identified in Part One). The search returned 12,255 records (for exact search string, see Supplementary Information F).

The two lists of sampled articles were each randomly shuffled using the R function ‘sample’ and the top 600 entries in each shuffled list were selected (to allow for exclusions). Articles were then evaluated sequentially until the target of 100 eligible articles in each sample was reached. Articles were excluded because they: (i) could not be accessed (n = 9); (ii) were non-English language (n = 4); (iii) indicated they had been retracted (n = 2), (iv) were non-empirical (n = 55), or (v) were conference abstracts or ‘supplement’ articles rather than complete articles (n = 9). Assessing whether an article is empirical involved some subjective judgement, but we used the following guidelines: empirical research must include some analysis of quantitative or qualitative data and the following do not count as empirical research: news, book reviews, editorials, previews, opinion pieces, simulations, proofs, theoretical papers, and reviews. We included primary research, secondary data analysis, and meta-analyses. If an article needed to be excluded after manual inspection, it was replaced with the next available article in its list, thus maintaining the random sampling scheme.

Note, the above applies to the Linked Method for calculating prevalence; for the Instance Method for calculating prevalence estimates, articles that were themselves post-publication critique (e.g. 'Letters') were included, but they did not count towards the 100 empirical articles sample size target.

### Procedure

*Piloting and training*

The data extraction procedures were piloted with 10 articles that were not included in the sample. All investigators were trained and familiarised with the data extraction process and practised with positive and negative training examples.

*Data extraction*

To code whether sampled articles were linked to post-publication critique (Linked Method), two investigators independently viewed each article on the journal’s website, assessed if the article met the eligibility criteria, and if it did, thoroughly checked the page for any linked post-publication critique.

To code whether the sampled article itself was an example of post-publication critique (Instance Method), the same two coders also read the title and, if necessary, the abstract of the sampled article.

Primary coding was conducted by AW (n = 246) and NM (n = 35). Secondary coding was conducted by BC (n = 122), SS (n = 110), and TEH (n = 49). Data extraction/classification disagreements were resolved by TEH, which was necessary for 15 articles (for details see Supplementary Information H).

## Results

```{r}
# Look for linked PPC in articles in random sample
random_linked <- d_articles %>%
  filter(sample_id == "random",
         ppc_linked == TRUE) %>%
  summarise(n = n())
```

```{r}
# Calculate CIs for linked PPC in articles in random sample

# Specify sample size for articles (same for both samples)
sample_size_a <- 101

ci_Rlinked <- binom.confint(random_linked$n, sample_size_a, method = "wilson", conf.level = confidence_level)

# Convert the lower and upper limits to percentages and round them to one decimal place
ci_randLinked_LL <- round(ci_Rlinked$lower*100, 1)
ci_randLinked_UL <- round(ci_Rlinked$upper*100, 1)
```

```{r}
# Look for instances of PPC in articles in random sample
random_instance <- d_articles %>%
  filter(sample_id == "random",
         exclude_reason == "IS ITSELF PPC") %>%
  summarise(n = n())
```

```{r}
# Calculate CIs for Instance PPC in random sample
ci_Rinstance <- binom.confint(random_instance$n, sample_size_a, method = "wilson", conf.level = confidence_level)

# Convert the lower and upper limits to percentages and round them to one decimal place
ci_randInstance_LL <- round(ci_Rinstance$lower*100, 1)
ci_randIinstance_UL <- round(ci_Rinstance$upper*100, 1)
```

Among the 101 empirical articles randomly sampled from all psychology journals, we did not find any examples of post-publication critiques based on the Linked Method nor the Instance Method, resulting in a prevalence estimate of `r random_linked`% (95% CI [`r ci_randInstance_LL`%, `r ci_randIinstance_UL`%]) for both methods.

```{r}
# Look for links to PPC in articles published in prominent journals
prominent_linked <- d_articles %>%
  filter(sample_id == "prominent",
         ppc_linked == TRUE) %>%
  summarise(n = n())
```

```{r}
ci_linked <- binom.confint(prominent_linked$n, sample_size_a, method = "wilson", conf.level = confidence_level)

# Convert the lower and upper limits to percentages and round them to one decimal place
ci_promLinked_LL <- round(ci_linked$lower*100, 1)
ci_promLinked_UL <- round(ci_linked$upper*100, 1)
```

```{r}
# Look for instances of PPC in articles published in prominent journals
prominent_instance <- d_articles %>%
  filter(sample_id == "prominent",
         exclude_reason == "IS ITSELF PPC") %>%
  summarise(n = n())
```

```{r}
# Calculate CIs for Instance PPC in prominent journals
ci_instance <- binom.confint(prominent_instance$n, sample_size_a, method = "wilson", conf.level = confidence_level)

# Convert the lower and upper limits to percentages and round them to one decimal place
ci_promInstance_LL <- round(ci_instance$lower*100, 1)
ci_promIinstance_UL <- round(ci_instance$upper*100, 1)
```

Among the 101 empirical articles randomly sampled from prominent psychology journals, we found only one post-publication critique using the Linked Method (i.e., one sampled article was associated with a linked post-publication critique), yielding a prevalence estimate of `r prominent_linked`% (95% CI [`r ci_promLinked_LL`%, `r `ci_promLinked_UL`%]). Using the Instance Method, we found no post-publication critiques (`r `prominent_instance`%, 95% CI [`r ci_promInstance_LL`%, `r ci_promIinstance_UL`%]) (i.e., none of the sampled articles were themselves instances of post-publication critique).   

# General Discussion

Post-publication critique is often viewed as an important mechanism of scientific self-correction [@robertk.merton1973; @vazire2022]. However, our study found that across the field of psychology only a minority of journals (`r randomPPC$count[randomPPC$has_ppc == "YES"]`, `r randomPPC$count[randomPPC$has_ppc == "YES"]`%, 95% CI [`r round(randomPPC_CI[1,5]*100)`%, `r round(randomPPC_CI[1,6]*100)`%]) had an explicit submission format for post-publication critique, and actual examples of post-publication critique are extremely rare. Prominent psychology journals were more likely to have an explicit format for post-publication critique according to their policies (38%), but the majority still did not. No journals in either sample supported web comments. Journals that did allow submission of post-publication critique sometimes imposed restrictions on length and time-to-submit. Occasionally, these limits were quite restrictive; the strictest length limit was 400 words and the strictest time-to-submit limit was 4 weeks after the target article was published. In practice, post-publication critiques were rare with only a single Linked example found within the articles sampled from prominent journals. Overall, our findings suggest that the policy and practice of post-publication critique is seriously neglected in psychology journals.

Should all journals offer an explicit avenue for post-publication critique? Post-publication critique is recognized as an important mechanism of scientific self-correction [@robertk.merton1973; @vazire2022] and facilitating critique is often considered to be a journal’s responsibility (e.g., according to @committeeonpublicationethics2021). Interestingly, in both the randomly selected and prominent samples, most (`r randomCOPE$COPEpercent[randomCOPE$has_ppc == "NO"]`% and `r promCOPE$COPEpercent[promCOPE$has_ppc == "NO"]`%, respectively) of the journals that did not offer post-publication critique were members of COPE^[COPE guidelines explicitly recommend that journals allow post-publication critique “on their site, through letters to the editor, or on external moderated website such as PubPeer” (COPE Council, 2021). Technically, a journal does not need to do anything to comply with this recommendation because PubPeer is an independent platform.]. It is unclear if publishers or journal editors have principled reasons for not explicitly offering post-publication critique. 

It could be argued that post-publication critique can take place in other contexts, such as social media or dedicated platforms like PubPeer [@bastian2014]; however, journals are in a privileged position to encourage, curate, and publish critiques in the scientific record, which improves      permanence and discoverability (i.e., by linking to critiques on the same webpage as the target article). Critiques posted informally on external platforms may be overlooked by journals and readers [@allison2016]. There is also a risk that third party sites will become defunct — as in the case of PubMed Commons, which was shut down in 2018 [ncbiinsights2018]. Some prominent psychologists have even argued that critiques should appear only in venues subject to editorial oversight (e.g., @fiske2016).

Are journal limits on post-publication critique justifiable? Length limits may promote concision; however, strict limits (e.g., a few hundred words) are likely to seriously reduce the scope and substance of critiques [@altman2005]. There is no obvious justification for imposing time-to-submit limits. Such limits imply that there is effectively a “statute of limitations” on scientific critique once a paper has been published for a given time (often no more than a few months). Only a few policies stated that critiques would be sent for independent external peer-review. It was unclear whether journals routinely send critiques to the original authors for review, but anecdotally, we know this can occur. Independent peer-review can provide more impartial assessment of critiques and bolster their legitimacy @schriger2010. Without independent review, the decision about whether to publish a critique may be based solely on the perspectives of individuals with a conflict of interest, namely the journal editors and the original authors.

Though our data indicate that post-publication critique is neglected in psychology journals, they do not provide insight into why this is the case. Potential reasons could be cultural (e.g., disciplines have different attitudes towards scientific criticism and how to handle it), pragmatic (e.g., some disciplines have a greater need for critique than others because of lower quality standards), bureaucratic (e.g., availability of resources to support post-publication critique), historic (e.g., particular individuals or events that have highlighted the value of post-publication critique), or reputational (e.g., a perception that post-publication critique could damage journal reputation). Other factors which may deter submission of post-publication critique include fear of retaliation, such as public defamation and legal threats from those whose research they questioned [@besancon2021].

Our research has several limitations. First, our operational definition of post-publication critique was intentionally focused on journal accountability and therefore some types of critical discourse (such as externally hosted critiques or criticisms embedded in research or review articles that do not target a specific article) were beyond the scope of this study. Our definition also excluded errata, corrections, and retractions, some of which may have been motivated by post-publication critiques that were eventually not published. Second, we relied on information explicitly published on journal websites; it is possible that some journals accept post-publication critique, even though they do not have an explicit policy saying so on their website. Third, it is possible that we missed some examples of post-publication critique because they were not clearly linked to the target article; we did our best to minimize this possibility by having two coders independently assess the presence of critique for each article. Fourth, our study focused on constraints  imposed by journals on post-publication critique; however, there are likely many contributing factors to the low prevalence of critique, such as authors’ ability and willingness to write and submit critiques, and the extent to which critiques are recognized in hiring, promotion, and funding decisions.

## Conclusion

Since 2011, psychology has been navigating a period of considerable epistemological turmoil, with serious concerns raised about research quality [@nosek2022]. Post-publication critique is a valuable mechanism for identifying problems in published research and highlighting them to readers [@hardwicke2022; @robertk.merton1973; @vazire2022]. The present study shows that post-publication critique is seriously neglected in policy and practice at psychology journals. Most journals do not have explicit formats for the submission of post-publication critiques and critiques are very rarely published. In Box 1, we provide some tentative policy suggestions that may help to improve this situation. 

**INSERT BOX 1**

## Author contributions

Conceptualization: T.E.H., A.W., S.V. Data curation: A.W. and T.E.H. Formal analysis: A.W. and T.E.H. Investigation: A.W., B.C., N.M., S.S., S.V, R.T.T., and T.E.H. Methodology: T.E.H. Project administration: A.W. and T.E.H. Resources: T.E.H. Software: T.E.H. Supervision: T.E.H. and S.V. Validation: A.W. and T.E.H. Visualization: A.W. Writing - original draft: A.W. Writing - review & editing: A.W., S.V., and T.E.H.   

## Acknowledgements

AW would like to thank her father, Robert Whamond, for providing technical assistance and support with software issues that arose during this project.

\newpage

## References